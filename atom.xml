<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Infomatic Technique Tutorial</title>
  
  <subtitle>Jack Gu&#39;s personal tech-tutorial</subtitle>
  <link href="/info-tech-tutorial/atom.xml" rel="self"/>
  
  <link href="https://3pigsgu.github.io/info-tech-tutorial/"/>
  <updated>2020-02-12T09:34:56.844Z</updated>
  <id>https://3pigsgu.github.io/info-tech-tutorial/</id>
  
  <author>
    <name>Jack Gu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用Bypy插件实现Linux系统中百度网盘的上传下载操作</title>
    <link href="https://3pigsgu.github.io/info-tech-tutorial/2020/02/12/other/linux_command_for_bypy/"/>
    <id>https://3pigsgu.github.io/info-tech-tutorial/2020/02/12/other/linux_command_for_bypy/</id>
    <published>2020-02-12T06:32:12.000Z</published>
    <updated>2020-02-12T09:34:56.844Z</updated>
    
    <content type="html"><![CDATA[<p>对于很多开发人员来说经常会面临大量数据需要备份的问题，比如数据库的bin-log数据日志，亦或软件包。而往往较大容量的备份数据从服务器上Download需要花费很长的数据，并且使用的Xftp文件传输工具也不可能持续稳定的长连接传输文件，另外连接中断后也并不能支持断点续传重试。因此，我们会想是否有替代的方式解决上面提到的问题。这篇文章则利用百度网盘来实现这个牛掰的备份功能。</p><p>大多数使用过百度网盘的人想必都有其免费提供的2TB存储空间。一般来讲，这个免费的存储容量足够用于个人的数据备份。那么如果在linux系统中把数据备份到百度网盘上，下面我们具体讲解百度云的一款Python客户端工具Bypy。</p><h3 id="1-Centos依赖安装"><a href="#1-Centos依赖安装" class="headerlink" title="1. Centos依赖安装"></a>1. Centos依赖安装</h3><p>在安装Bypy软件工具之前，这里需要稍微提一下一些注意点。对于Centos 7操作系统在安装pip之前，需要先如下安装依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># yum -y install epel-release</span><br></pre></td></tr></table></figure><h3 id="2-安装软件工具"><a href="#2-安装软件工具" class="headerlink" title="2. 安装软件工具"></a>2. 安装软件工具</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># yum -y install python-pip</span><br><span class="line"># pip install requests</span><br><span class="line"># pip install bypy</span><br></pre></td></tr></table></figure><h3 id="3-授权登陆"><a href="#3-授权登陆" class="headerlink" title="3. 授权登陆"></a>3. 授权登陆</h3><p>执行 bypy info，显示下边信息，根据提示，通过浏览器访问下边灰色的https链接，如果此时百度网盘账号正在登陆，会出现长串授权码，复制。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># bypy info</span><br><span class="line">Please visit:   # 访问下边这个连接，复制授权码</span><br><span class="line">https:&#x2F;&#x2F;openapi.baidu.com&#x2F;oauth&#x2F;2.0&#x2F;authorize?scope&#x3D;basic+netdisk&amp;redirect_uri&#x3D;oob&amp;response_type&#x3D;code&amp;client_id&#x3D;q8WE4EpCsau1oS0MplgMKNBn</span><br><span class="line">And authorize this app</span><br><span class="line">Paste the Authorization Code here within 10 minutes.</span><br><span class="line">Press [Enter] when you are done    # 提示在下边粘贴授权码</span><br></pre></td></tr></table></figure><p>在下边图示红色位置粘贴授权码，耐心等待一会即可(1-2分钟)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Press [Enter] when you are done</span><br><span class="line">a288f3d775fa905a6911692a0808f6a8</span><br><span class="line">Authorizing, please be patient, it may take upto None seconds...</span><br><span class="line">Authorizing&#x2F;refreshing with the OpenShift server ...</span><br><span class="line">OpenShift server failed, authorizing&#x2F;refreshing with the Heroku server ...</span><br><span class="line">Successfully authorized</span><br><span class="line">Quota: 2.015TB</span><br><span class="line">Used: 740.493GB</span><br></pre></td></tr></table></figure><p>授权成功。</p><h3 id="4-上传文件到云盘"><a href="#4-上传文件到云盘" class="headerlink" title="4. 上传文件到云盘"></a>4. 上传文件到云盘</h3><p>由于百度PCS API权限限制，程序只能存取百度云端/apps/bypy目录下面的文件和目录。我们可以通过：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># bypy list</span><br><span class="line">&#x2F;apps&#x2F;bypy ($t $f $s $m $d):</span><br></pre></td></tr></table></figure><p>把本地当前目录下的文件同步到百度云盘：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bypy upload &#x2F;文件路径&#x2F;文件名  &#x2F;网盘路径</span><br></pre></td></tr></table></figure><p>把云盘上的内容同步到本地:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bypy downdir</span><br></pre></td></tr></table></figure><p>比较本地当前目录和云盘根目录，看是否一致，来判断是否同步成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bypy compare</span><br></pre></td></tr></table></figure><p>把当前目录同步到云盘</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bypy syncup</span><br></pre></td></tr></table></figure><p>PS: 运行时添加-v参数，会显示进度详情。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;对于很多开发人员来说经常会面临大量数据需要备份的问题，比如数据库的bin-log数据日志，亦或软件包。而往往较大容量的备份数据从服务器上Download需要花费很长的数据，并且使用的Xftp文件传输工具也不可能持续稳定的长连接传输文件，另外连接中断后也并不能支持断点续传重试
      
    
    </summary>
    
    
      <category term="其他" scheme="https://3pigsgu.github.io/info-tech-tutorial/categories/Other/"/>
    
    
      <category term="其他" scheme="https://3pigsgu.github.io/info-tech-tutorial/tags/Other/"/>
    
  </entry>
  
  <entry>
    <title>Apache Spark Standalone集群模式部署</title>
    <link href="https://3pigsgu.github.io/info-tech-tutorial/2020/01/14/spark-tutorial-01/"/>
    <id>https://3pigsgu.github.io/info-tech-tutorial/2020/01/14/spark-tutorial-01/</id>
    <published>2020-01-14T09:20:19.000Z</published>
    <updated>2020-01-21T07:57:48.311Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1. 环境准备"></a>1. 环境准备</h2><p>为了部署Apache Spark Standalone集群模式，本次准备了三台Linux虚拟机来实现集群：一台Master和两台Slaves。<br>Master: 192.168.71.130<br>Slave: 192.168.71.131<br>Slave: 192.168.71.132<br>为了保证三台虚拟机之间能正常访问通讯，需要在每台虚拟机上配置/etc目录下的hosts和hostname文件。<br>所有的hosts文件设置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop0 192.168.71.130</span><br><span class="line">hadoop1 192.168.71.131</span><br><span class="line">hadoop2 192.168.71.132</span><br></pre></td></tr></table></figure><p>各自对应IP的虚拟机上的hostname设置成上面相对应的IP地址前面的别名，例如IP为192.168.71.130的虚拟机的hostname设置成hadoop0。然后，通过ping命令测试各节点之间是否互通。<br>如果各节点之间有使用密钥验证登录，那么需要额外的生成密钥并添加到各节点认证文件中，以实现各节点的免密登录。<br>生成密钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><p>一路默认回车，这样在当前用户的主目录下会生成.ssh文件夹。进入这个文件夹，首先把本机的公钥加入认证文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat id_rsa.pub &gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure><h2 id="2-下载Spark"><a href="#2-下载Spark" class="headerlink" title="2. 下载Spark"></a>2. 下载Spark</h2><p>接下来，我们去Apache Spark官网下载相对应的Spark Pack。这里我们选择2.4.4 release版本并预编译包含Hadoop2.7的lib库的Spark。(<a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">Spark下载地址</a>)</p><img src="/info-tech-tutorial/2020/01/14/spark-tutorial-01/spark-download.png" class=""><p>这里我们选择包含hadoop的lib库的Spark是为了免去配置Spark所需的hadoop依赖库的繁琐过程，具体的不带hadoop lib库的Spark的手动配置过程，会在Spark与Hadoop的整合应用这一章再展开。<br>然后点击下载链接，进入跳转页面后，任意选择一个下载链接。接着在预先准备的Linux虚拟机上下载Spark Package, 例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;apache&#x2F;spark&#x2F;spark-2.4.4&#x2F;spark-2.4.4-bin-hadoop2.7.tgz</span><br><span class="line">tar zxvf spark-2.4.4-bin-hadoop2.7.tgz</span><br></pre></td></tr></table></figure><p>在/etc/profile中添加SPARK_HOME路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line">export SPARK_HOME&#x3D;&#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;home&#x2F;guty&#x2F;software&#x2F;jdk1.8.0_221</span><br><span class="line">export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$SPARK_HOME&#x2F;bin:$PATH</span><br><span class="line"></span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><h2 id="3-配置Spark-Master"><a href="#3-配置Spark-Master" class="headerlink" title="3. 配置Spark Master"></a>3. 配置Spark Master</h2><p>在准备好Spark Package之后，我们上Master节点（192.168.71.130），进行相应的配置。<br>首先我们进入Spark目录spark-2.4.4-bin-hadoop2.7，然后配置spark环境变量,修改conf/spark-env.sh文件,在文件末尾添加JAVA_HOME路径（指定所使用的JDK）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp conf&#x2F;spark-env.sh.template conf&#x2F;spark-env.sh</span><br><span class="line">vi conf&#x2F;spark_env.sh</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;home&#x2F;guty&#x2F;software&#x2F;jdk1.8.0_221</span><br></pre></td></tr></table></figure><p>接着配置slave机器的信息,修改conf/slaves文件,在文件末尾添加集群各台主机的hostname：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp conf&#x2F;slaves.template conf&#x2F;slaves</span><br><span class="line">vi conf&#x2F;slaves</span><br><span class="line">hadoop0</span><br><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br></pre></td></tr></table></figure><h2 id="4-配置Spark-Workers-Slaves"><a href="#4-配置Spark-Workers-Slaves" class="headerlink" title="4. 配置Spark Workers (Slaves)"></a>4. 配置Spark Workers (Slaves)</h2><p>当Spark Master配置完成后，配置各个Spark Workers相对比较简单。这里使用linux的scp命令直接在各主机节点的局域网内床送复制SparK Pack程序文件。<br>例如在节点192.168.71.131和192.168.71.132上执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp root@192.168.71.130:&#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7  &#x2F;home&#x2F;guty&#x2F;software</span><br></pre></td></tr></table></figure><p>同样，在节点192.168.71.131和192.168.71.132上/etc/profile文件中添加SPARK_HOME路径。</p><h2 id="5-启动和停止Spark"><a href="#5-启动和停止Spark" class="headerlink" title="5. 启动和停止Spark"></a>5. 启动和停止Spark</h2><ul><li><p>启动Spark<br>启动后会显示如下信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;sbin&#x2F;start-all.sh</span><br><span class="line"></span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to &#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.master.Master-1-hadoop0.out</span><br><span class="line">hadoop2: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-hadoop2.out</span><br><span class="line">hadoop1: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-hadoop1.out</span><br><span class="line">hadoop0: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-hadoop0.out</span><br></pre></td></tr></table></figure><p>然后可以访问Spark Web UI界面，<a href="http://192.168.71.130:8080" target="_blank" rel="noopener">http://192.168.71.130:8080</a>是Spark Master节点web界面，下图可以看到各个workers节点的列表信息：</p><img src="/info-tech-tutorial/2020/01/14/spark-tutorial-01/spark-master.png" class=""><p>单击上图其中某一个worker节点，可以查看该worker节点的任务执行情况，如下图：</p><img src="/info-tech-tutorial/2020/01/14/spark-tutorial-01/spark-worker.png" class=""></li><li><p>停止Spark<br>停止Spark会显示如下信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;sbin&#x2F;stop-all.sh </span><br><span class="line"></span><br><span class="line">hadoop1: stopping org.apache.spark.deploy.worker.Worker</span><br><span class="line">hadoop0: stopping org.apache.spark.deploy.worker.Worker</span><br><span class="line">hadoop2: stopping org.apache.spark.deploy.worker.Worker</span><br><span class="line">stopping org.apache.spark.deploy.master.Master</span><br></pre></td></tr></table></figure></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">Spark Standalone Mode</a></li><li><a href="http://jianshu.com/p/2103ba93a2dd" target="_blank" rel="noopener">Spark的独立集群模式部署</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-环境准备&quot;&gt;&lt;a href=&quot;#1-环境准备&quot; class=&quot;headerlink&quot; title=&quot;1. 环境准备&quot;&gt;&lt;/a&gt;1. 环境准备&lt;/h2&gt;&lt;p&gt;为了部署Apache Spark Standalone集群模式，本次准备了三台Linux虚拟机来实现集群
      
    
    </summary>
    
    
      <category term="Spark实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/categories/Spark-Action/"/>
    
    
      <category term="框架实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"/>
    
  </entry>
  
  <entry>
    <title>Apache Spark实战全集</title>
    <link href="https://3pigsgu.github.io/info-tech-tutorial/2020/01/14/spark/spark-tutorial-00/"/>
    <id>https://3pigsgu.github.io/info-tech-tutorial/2020/01/14/spark/spark-tutorial-00/</id>
    <published>2020-01-14T03:21:24.000Z</published>
    <updated>2020-01-14T09:22:42.960Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Apache-Spark简介"><a href="#1-Apache-Spark简介" class="headerlink" title="1. Apache Spark简介"></a>1. Apache Spark简介</h2><p>Apache Spark是一款轻量级快速统一分析引擎。作为一个开源的大数据处理框架，Spark围绕速度，易用性和复杂分析能力而构建。开发人员在处理大量数据或处理内存有限和时间过长时通常会使用Spark。</p><h3 id="1-1-超快的速度"><a href="#1-1-超快的速度" class="headerlink" title="1.1. 超快的速度"></a>1.1. 超快的速度</h3><p>Apache Spark在批量数据和流量数据处理方面都实现了高性能，并提供了优异的调度器、查询器和物理执行引擎给大家使用。例如下图，Spark比Hadoop运行工作负载的速度提高了100倍。</p><img src="/info-tech-tutorial/2020/01/14/spark/spark-tutorial-00/spark-logistic-regression.png" class="" title="Logistic regression in Hadoop and Spark"><h3 id="1-2-简单易用"><a href="#1-2-简单易用" class="headerlink" title="1.2. 简单易用"></a>1.2. 简单易用</h3><p>Spark提供了80多个高级操作器，可轻松构建并行应用程序。您可以使用Scala，Python，R和SQL Shell与Spark进行交互。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.json(<span class="string">"logs.json"</span>) df.where(<span class="string">"age &gt; 21"</span>)</span><br><span class="line">        .select(<span class="string">"name.first"</span>).show()</span><br></pre></td></tr></table></figure><h3 id="1-3-通用性"><a href="#1-3-通用性" class="headerlink" title="1.3. 通用性"></a>1.3. 通用性</h3><p>Spark可以结合SQL，流和复杂的分析一起使用。Spark为包括SQL和DataFrames，用于机器学习的MLlib，GraphX和Spark Streaming在内的库提供了强大的支持。您可以在同一应用程序中无缝组合这些库。</p><img src="/info-tech-tutorial/2020/01/14/spark/spark-tutorial-00/spark-stack.png" class=""><h2 id="1-4-随处运行"><a href="#1-4-随处运行" class="headerlink" title="1.4. 随处运行"></a>1.4. 随处运行</h2><p>Spark可在Hadoop，Apache Mesos，Kubernetes，独立或云中运行。它可以访问各种数据源。您可以在EC2，Hadoop YARN，Mesos或Kubernetes上使用独立集群模式运行Spark。访问HDFS，Alluxio，Apache Cassandra，Apache HBase，Apache Hive和数百种其他数据源中的数据。</p><img src="/info-tech-tutorial/2020/01/14/spark/spark-tutorial-00/spark-runs-everywhere.png" class=""><h2 id="2-Spark实战章节"><a href="#2-Spark实战章节" class="headerlink" title="2. Spark实战章节"></a>2. Spark实战章节</h2><p>第一篇： <a href="/info-tech-tutorial/2020/01/14/spark-tutorial-01/" title="Apache Spark Standalone集群模式部署">Apache Spark Standalone集群模式部署</a></p><h2 id="Reference-List"><a href="#Reference-List" class="headerlink" title="Reference List"></a>Reference List</h2><ol><li><a href="http://spark.apache.org/" target="_blank" rel="noopener">Apache Spark官网</a></li><li><a href="http://spark.apache.org/docs/latest/" target="_blank" rel="noopener">Apache Spark Documentation</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Apache-Spark简介&quot;&gt;&lt;a href=&quot;#1-Apache-Spark简介&quot; class=&quot;headerlink&quot; title=&quot;1. Apache Spark简介&quot;&gt;&lt;/a&gt;1. Apache Spark简介&lt;/h2&gt;&lt;p&gt;Apache Spark
      
    
    </summary>
    
    
      <category term="Spark实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/categories/Spark-Action/"/>
    
    
      <category term="框架实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus+Grafana监控平台搭建与配置</title>
    <link href="https://3pigsgu.github.io/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/"/>
    <id>https://3pigsgu.github.io/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/</id>
    <published>2020-01-07T07:19:02.000Z</published>
    <updated>2020-01-07T15:35:28.831Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Prometheus简介"><a href="#1-Prometheus简介" class="headerlink" title="1. Prometheus简介"></a>1. Prometheus简介</h2><p>Prometheus是最初在SoundCloud上构建的开源系统监视和警报工具包。自2012年成立以来，许多公司和组织都采用了Prometheus，该项目拥有非常活跃的开发人员和用户社区。现在，它是一个独立的开源项目，并且独立于任何公司进行维护。为了强调这一点并阐明项目的治理结构，Prometheus于2016年加入了Cloud Native Computing Foundation，这是继Kubernetes之后的第二个托管项目。 </p><h3 id="1-1-Prometheus特征"><a href="#1-1-Prometheus特征" class="headerlink" title="1.1. Prometheus特征"></a>1.1. Prometheus特征</h3><p>普罗米修斯的主要特点是： </p><ul><li>一个多维数据模型，其中包含通过度量标准名称和键/值对标识的时间序列数据；</li><li>具备PromQL查询语言，是一种灵活的可利用维度的查询语言；</li><li>不依赖分布式存储；单服务器节点自治；</li><li>通过HTTP上的拉取模型数据进行时间序列收集；</li><li>通过中间网关支持推送时间序列；</li><li>通过服务发现或静态配置发现目标；</li><li>多种图形和仪表板支持模式。</li></ul><h3 id="1-2-Prometheus组件构成"><a href="#1-2-Prometheus组件构成" class="headerlink" title="1.2. Prometheus组件构成"></a>1.2. Prometheus组件构成</h3><p>普罗米修斯的生态系统是由各种不同的组件构成，其中组件都是可选的：</p><ul><li>Prometheus Server主服务器，它会抓取并存储时间序列数据；</li><li>Client Library客户端库提供了可检索的应用代码；</li><li>Push gateway推送网关支持短生命周期的Job工作任务；</li><li>特定的Exporters提供了多种HAProxy， StatsD，Graphite等定制服务；</li><li>Alertmanager提供了告警的控制；</li><li>并兼容多种辅助工具。</li></ul><h3 id="1-3-Prometheus体系架构"><a href="#1-3-Prometheus体系架构" class="headerlink" title="1.3. Prometheus体系架构"></a>1.3. Prometheus体系架构</h3><p>下图说明了Prometheus的体系结构及其某些生态系统组件：</p><img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/prometheus_architecture.png" class=""><p>Prometheus直接或通过中间推送网关从已检测作业中删除指标，以用于短期作业。它在本地存储所有报废的样本，并对这些数据运行规则，以汇总和记录现有数据中的新时间序列，或生成警报。 Grafana或其他API使用者可用于可视化收集的数据。</p><h2 id="2-安装配置Prometheus"><a href="#2-安装配置Prometheus" class="headerlink" title="2. 安装配置Prometheus"></a>2. 安装配置Prometheus</h2><p>下面介绍如何安装Prometheus，并且利用两个exporter去监控Linux server和Mysql数据库。这里需要用到node_exporter和mysqld_exporter两个收集器。</p><ul><li>首先我们下载Prometheus Server，并使用prometheus.yml配置文件启动服务:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ wget https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;releases&#x2F;download&#x2F;v2.15.2&#x2F;prometheus-2.15.2.linux-amd64.tar.gz</span><br><span class="line">$ tar zxvf prometheus-2.15.2.linux-amd64.tar.gz</span><br><span class="line">$ cd prometheus-2.15.2.linux-amd64</span><br><span class="line">$ .&#x2F;prometheus --config.file&#x3D;prometheus.yml</span><br></pre></td></tr></table></figure>Prometheus本身提供web界面。当Prometheus Server启动起来后，可通过浏览器访问地址<a href="http://localhost:9090/graph" target="_blank" rel="noopener">http://localhost:9090/graph</a> 来进入Prometheus web界面。<img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/prometheus_web.png" class=""></li><li>接着下载并安装node_exporter收集器来收集Linux Server性能数据：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ wget https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;node_exporter&#x2F;releases&#x2F;download&#x2F;v0.18.1&#x2F;node_exporter-0.18.1.linux-amd64.tar.gz</span><br><span class="line">$ tar zxvf node_exporter-0.18.1.linux-amd64.tar.gz</span><br><span class="line">$ cd node_exporter-0.18.1.linux-amd64</span><br><span class="line">$ .&#x2F;node_exporter</span><br></pre></td></tr></table></figure></li><li>最后下载并安装mysqld_exporter收集器来收集Mysql性能数据，在安装收集器之前需要先在数据库新建用户以提供mysqld_exporter收集器所需的Mysql权限：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GRANT REPLICATION CLIENT,PROCESS ON *.* TO &#39;mysql_monitor&#39;@&#39;localhost&#39; identified by &#39;mysql_monitor&#39;;</span><br><span class="line">GRANT SELECT ON *.* TO &#39;mysql_monitor&#39;@&#39;localhost&#39;;</span><br></pre></td></tr></table></figure>安装mysqld_exporter收集器，并在mysqld_exporter-0.12.1.linux-amd64解压缩后的目录下新增一个名为.my.cnf文件：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ wget https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;mysqld_exporter&#x2F;releases&#x2F;download&#x2F;v0.12.1&#x2F;mysqld_exporter-0.12.1.linux-amd64.tar.gz</span><br><span class="line">$ tar zxvf mysqld_exporter-0.12.1.linux-amd64.tar.gz</span><br><span class="line">$ cd mysqld_exporter-0.12.1.linux-amd64</span><br><span class="line"></span><br><span class="line">$ cat &#x2F;usr&#x2F;local&#x2F;mysqld_exporter-0.10.0.linux-amd64&#x2F;.my.cnf</span><br><span class="line">[client]</span><br><span class="line">user&#x3D;mysql_monitor</span><br><span class="line">password&#x3D;mysql_monitor</span><br><span class="line"></span><br><span class="line">.&#x2F;mysqld_exporter --config.my-cnf&#x3D;&quot;.my.cnf&quot;</span><br></pre></td></tr></table></figure></li><li>最后为了能在Prometheus Server服务器端能够接收node_exporter和mysqld_exporter采集到的数据，需要在prometheus.yml中新增如下配置：<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="comment"># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">'prometheus'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># metrics_path defaults to '/metrics'</span></span><br><span class="line">    <span class="comment"># scheme defaults to 'http'.</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> <span class="string">['localhost:9090']</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">'server'</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> <span class="string">['localhost:9100']</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">'mysql'</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> <span class="string">['localhost:9104']</span></span><br></pre></td></tr></table></figure>重启Prometheus Server应用后，在Prometheus Web界面中的Status-&gt;Targets页面，可以看到Mysql和L两个Target的状态已经变成UP了：<img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/prometheus_targets.png" class="">更多参考：<a href="https://ryanyang.gitbook.io/prometheus/" target="_blank" rel="noopener">Prometheus中文文档</a></li></ul><h2 id="3-Grafana安装配置"><a href="#3-Grafana安装配置" class="headerlink" title="3. Grafana安装配置"></a>3. Grafana安装配置</h2><p>Grafana是一款开源的应用分析和监控解决方案。由于Prometheus Web的界面过于简单，为了能够更绚丽的展示Prometheus监控的应用数据，这里选用Grafana实现监控数据的动态可视化组合Dashboard。</p><ul><li>首先先下载并运行Grafana：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ wget https:&#x2F;&#x2F;dl.grafana.com&#x2F;oss&#x2F;release&#x2F;grafana-6.5.2.linux-amd64.tar.gz </span><br><span class="line">$ tar -zxvf grafana-6.5.2.linux-amd64.tar.gz</span><br><span class="line">$ cd grafana-6.5.2.linux-amd64</span><br><span class="line">$ .&#x2F;bin&#x2F;grafana-server web</span><br></pre></td></tr></table></figure>Grafana启动后，我们可通过<a href="http://monitor_host:3000" target="_blank" rel="noopener">http://monitor_host:3000</a> 访问Grafana网页界面（默认登陆帐号/密码为admin/admin）：<img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/grafana_dashboard.png" class=""></li><li>接着，新建一个Data Source以从Prometheus接收数据：<img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/grafana_datasource.png" class=""></li><li>然后新建两个Dashboard面板来动态展示Linux和Mysql的监控情况，这里我们可以从Grafana Lab上下载相关应用的Dashboard面板现成样式。<br>Server Dashboard模板：<img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/grafana_dashboard_node.png" class="" title="Server Dashboard Style">Mysql Dashboard模板：<img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/grafana_dashboard_mysql.png" class="" title="Mysql Dashboard Style">这里我们只要复制对应的Dashboard的编号，比如上图中Server Dashboard编号是11074和Mysql Dashboard编号是6239。</li><li>接下来，在Grafana的Import界面粘贴对应的编号，然后生成Unique identifier (uid)和配置Prometheus Data Source：<img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/grafana_import.png" class=""></li><li>最后，生成的Dashboard界面如下：<br>Server Dashboard界面：<img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/grafana_server_interface.png" class="">Mysql Dashboard界面：<img src="/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/grafana_mysql_interface.png" class=""></li></ul><h2 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h2><p>至此，整个Prometheus+Grafana监控平台搭建与配置完成。如有疑问欢迎留言！</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://prometheus.io/docs/introduction/overview/" target="_blank" rel="noopener">Prometheus DOCS</a>;</li><li><a href="https://blog.csdn.net/qq_36357820/article/details/80777167" target="_blank" rel="noopener">prometheus+grafana监控设置</a>;</li><li><a href="https://www.jianshu.com/p/8d2c020313f0" target="_blank" rel="noopener">Prometheus+Grafana监控系统搭建</a>.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Prometheus简介&quot;&gt;&lt;a href=&quot;#1-Prometheus简介&quot; class=&quot;headerlink&quot; title=&quot;1. Prometheus简介&quot;&gt;&lt;/a&gt;1. Prometheus简介&lt;/h2&gt;&lt;p&gt;Prometheus是最初在SoundC
      
    
    </summary>
    
    
      <category term="其他" scheme="https://3pigsgu.github.io/info-tech-tutorial/categories/Other/"/>
    
    
      <category term="其他" scheme="https://3pigsgu.github.io/info-tech-tutorial/tags/Other/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink三类API接口</title>
    <link href="https://3pigsgu.github.io/info-tech-tutorial/2019/12/30/flink-tutorial-02/"/>
    <id>https://3pigsgu.github.io/info-tech-tutorial/2019/12/30/flink-tutorial-02/</id>
    <published>2019-12-30T07:46:29.000Z</published>
    <updated>2020-01-04T17:19:41.566Z</updated>
    
    <content type="html"><![CDATA[<p>本篇主要从Flink的三类API接口（DataStream API, DataSet API和Table API）去演示Word Count应用。希望能通过这三个示例代码帮助大家更好的理解这三类API接口。</p><h2 id="1-DataStream-API接口"><a href="#1-DataStream-API接口" class="headerlink" title="1. DataStream API接口"></a>1. DataStream API接口</h2><p>DataStream API接口即是Flink流式处理接口，对无限制数据流进行过滤或聚合等转换。</p><h3 id="1-1-主要依赖"><a href="#1-1-主要依赖" class="headerlink" title="1.1. 主要依赖"></a>1.1. 主要依赖</h3><p>对于DataStream API接口主要依赖flink-streaming-java_2.11这个jar包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-streaming-java_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="1-2-代码实现"><a href="#1-2-代码实现" class="headerlink" title="1.2. 代码实现"></a>1.2. 代码实现</h3><p>首先，我们要创建一个StreamExecutionEnvironment env流式运行环境；<br>然后，获取要输入的数据流的源数据DataStreamSource source，这里模拟输入一段文字；<br>接着，对流式数据进行实时数据处理，通过表达式”\W+”拆解单个单词并构造成2-tuples（二元组元）集合；再者通过keyBy( 0 )和sum( 1 )对二元组按照下标0的元素分组和下标1的元素求和；<br>最后，通过print()打印出最终流式数据处理结果。<br>这里最重要的是所有上面构建的过程都需要通过env.execute()来开启任务的执行。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.typeutils.TupleTypeInfo;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountStreaming</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// set up the execution environment</span></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// get input data</span></span><br><span class="line">        DataStreamSource&lt;String&gt; source = env.fromElements(</span><br><span class="line">                <span class="string">"To be, or not to be,--that is the question:--"</span>,</span><br><span class="line">                <span class="string">"Whether 'tis nobler in the mind to suffer"</span>,</span><br><span class="line">                <span class="string">"The slings and arrows of outrageous fortune"</span>,</span><br><span class="line">                <span class="string">"Or to take arms against a sea of troubles"</span></span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        source</span><br><span class="line">                <span class="comment">// split up the lines in pairs (2-tuples) containing: (word,1)</span></span><br><span class="line">                .flatMap( ( String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out ) -&gt; &#123;</span><br><span class="line">                    <span class="comment">// emit the pairs</span></span><br><span class="line">                    <span class="keyword">for</span>( String token : value.toLowerCase().split( <span class="string">"\\W+"</span> ) )&#123;</span><br><span class="line">                        <span class="keyword">if</span>( token.length() &gt; <span class="number">0</span> )&#123;</span><br><span class="line">                            out.collect( <span class="keyword">new</span> Tuple2&lt;&gt;( token, <span class="number">1</span> ) );</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; )</span><br><span class="line">                <span class="comment">// due to type erasure, we need to specify the return type</span></span><br><span class="line">                .returns( TupleTypeInfo.getBasicTupleTypeInfo( String<span class="class">.<span class="keyword">class</span>, <span class="title">Integer</span>.<span class="title">class</span> ) )</span></span><br><span class="line">                // group by the tuple field "0"</span><br><span class="line">                .keyBy( <span class="number">0</span> )</span><br><span class="line">                <span class="comment">// sum up tuple on field "1"</span></span><br><span class="line">                .sum( <span class="number">1</span> )</span><br><span class="line">                <span class="comment">// print the result</span></span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// start the job</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-DataSet-API接口"><a href="#2-DataSet-API接口" class="headerlink" title="2. DataSet API接口"></a>2. DataSet API接口</h2><p>DataSet API接口即是Flink批量处理接口，可批量对数据集进行转换。</p><h3 id="2-1-主要依赖"><a href="#2-1-主要依赖" class="headerlink" title="2.1. 主要依赖"></a>2.1. 主要依赖</h3><p>对于DataSet API接口主要依赖flink-java和flink-clients_2.11这个两个jar包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-clients_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="2-2-代码实现"><a href="#2-2-代码实现" class="headerlink" title="2.2. 代码实现"></a>2.2. 代码实现</h3><p>整个代码构造过程与DataStream流式处理的过程一致，区别在于这里构造的执行环境是ExecutionEnvironment env，输入的是数据集DataSet text。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.aggregation.Aggregations;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// set up the execution environment</span></span><br><span class="line">        <span class="keyword">final</span> ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// input data</span></span><br><span class="line">        <span class="comment">// you can also use env.readTextFile(...) to get words</span></span><br><span class="line">        DataSet&lt;String&gt; text = env.fromElements(</span><br><span class="line">                <span class="string">"To be, or not to be,--that is the question:--"</span>,</span><br><span class="line">                <span class="string">"Whether 'tis nobler in the mind to suffer"</span>,</span><br><span class="line">                <span class="string">"The slings and arrows of outrageous fortune"</span>,</span><br><span class="line">                <span class="string">"Or to take arms against a sea of troubles,"</span></span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts =</span><br><span class="line">                <span class="comment">// split up the lines in pairs (2-tuples) containing: (word,1)</span></span><br><span class="line">                text.flatMap( <span class="keyword">new</span> LineSplitter() )</span><br><span class="line">                        <span class="comment">// group by the tuple field "0" and sum up tuple field "1"</span></span><br><span class="line">                        .groupBy( <span class="number">0</span> )</span><br><span class="line">                        .aggregate( Aggregations.SUM, <span class="number">1</span> );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// emit result</span></span><br><span class="line">        counts.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>数据处理中构造了一个LineSplitter类去实现与Flink流式处理接口相同的工作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">( String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out )</span></span>&#123;</span><br><span class="line">        <span class="comment">// normalize and split the line into words</span></span><br><span class="line">        String[] tokens = value.toLowerCase().split( <span class="string">"\\W+"</span> );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// emit the pairs</span></span><br><span class="line">        <span class="keyword">for</span>( String token : tokens )&#123;</span><br><span class="line">            <span class="keyword">if</span>( token.length() &gt; <span class="number">0</span> )&#123;</span><br><span class="line">                out.collect( <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;( token, <span class="number">1</span> ) );</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-Table-API接口"><a href="#3-Table-API接口" class="headerlink" title="3. Table API接口"></a>3. Table API接口</h2><p>Table API接口即是Flink类似于SQL的表达语言的数据处理接口，可以嵌入批处理和流应用程序中一起使用。</p><h3 id="3-1-主要依赖"><a href="#3-1-主要依赖" class="headerlink" title="3.1. 主要依赖"></a>3.1. 主要依赖</h3><p>对于Table API接口主要依赖下列这些jar包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-table-api-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-table-common&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-table-planner_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.9.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="3-2-代码实现"><a href="#3-2-代码实现" class="headerlink" title="3.2. 代码实现"></a>3.2. 代码实现</h3><p>整个代码构造过程与前面两种处理方式的过程其实是一致的，不同点在于这中间需要实现BatchTableEnvironment表操作环境，而处理完的数据（即拆分后单词）会存储在创建的”words”表中，其字段名称叫做”word”。<br>最后通过SQL语句的方式从”word”表中查询汇总单词数量。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.FlatMapOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountTable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// set up the execution environment</span></span><br><span class="line">        <span class="keyword">final</span> ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">//final BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);</span></span><br><span class="line">        <span class="keyword">final</span> BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env, TableConfig.getDefault());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// get input data</span></span><br><span class="line">        DataSource&lt;String&gt; source = env.fromElements(</span><br><span class="line">                <span class="string">"To be, or not to be,--that is the question:--"</span>,</span><br><span class="line">                <span class="string">"Whether 'tis nobler in the mind to suffer"</span>,</span><br><span class="line">                <span class="string">"The slings and arrows of outrageous fortune"</span>,</span><br><span class="line">                <span class="string">"Or to take arms against a sea of troubles"</span></span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// split the sentences into words</span></span><br><span class="line">        FlatMapOperator&lt;String, String&gt; dataset = source</span><br><span class="line">                .flatMap( ( String value, Collector&lt;String&gt; out ) -&gt; &#123;</span><br><span class="line">                    <span class="keyword">for</span>( String token : value.toLowerCase().split( <span class="string">"\\W+"</span> ) )&#123;</span><br><span class="line">                        <span class="keyword">if</span>( token.length() &gt; <span class="number">0</span> )&#123;</span><br><span class="line">                            out.collect( token );</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; )</span><br><span class="line">                <span class="comment">// with lambdas, we need to tell flink what type to expect</span></span><br><span class="line">                .returns(String<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// create a table named "words" from the dataset</span></span><br><span class="line">        tableEnv.registerDataSet( <span class="string">"words"</span>, dataset, <span class="string">"word"</span> );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// word count using an sql query</span></span><br><span class="line">        Table results = tableEnv.sqlQuery( <span class="string">"select word, count(*) from words group by word"</span> );</span><br><span class="line">        tableEnv.toDataSet( results, Row<span class="class">.<span class="keyword">class</span> ).<span class="title">print</span>()</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4-代码执行"><a href="#4-代码执行" class="headerlink" title="4. 代码执行"></a>4. 代码执行</h2><p>使用flink命令行工具（在flink安装的bin文件夹中）启动程序：</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink run -c your.package.WordCount target/your-jar.jar</span><br></pre></td></tr></table></figure><p>-c选项允许您指定要运行的类。如果jar是可执行的/定义了主类，则没有必要。<br>运行结果如下：</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">(a,1)</span><br><span class="line">(against,1)</span><br><span class="line">(and,1)</span><br><span class="line">(arms,1)</span><br><span class="line">(arrows,1)</span><br><span class="line">(be,2)</span><br><span class="line">(fortune,1)</span><br><span class="line">(in,1)</span><br><span class="line">(is,1)</span><br><span class="line">(mind,1)</span><br><span class="line">(nobler,1)</span><br><span class="line">(not,1)</span><br><span class="line">(of,2)</span><br><span class="line">(or,2)</span><br><span class="line">(outrageous,1)</span><br><span class="line">(question,1)</span><br><span class="line">(sea,1)</span><br><span class="line">(slings,1)</span><br><span class="line">(suffer,1)</span><br><span class="line">(take,1)</span><br><span class="line">(that,1)</span><br><span class="line">(the,3)</span><br><span class="line">(tis,1)</span><br><span class="line">(to,4)</span><br><span class="line">(troubles,1)</span><br><span class="line">(whether,1)</span><br></pre></td></tr></table></figure><h2 id="References："><a href="#References：" class="headerlink" title="References："></a>References：</h2><ol><li><a href="https://riptutorial.com/apacheflink/topic/5798/getting-started-with-apache-flink" target="_blank" rel="noopener">Getting Started with Apache-Flink</a>.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇主要从Flink的三类API接口（DataStream API, DataSet API和Table API）去演示Word Count应用。希望能通过这三个示例代码帮助大家更好的理解这三类API接口。&lt;/p&gt;
&lt;h2 id=&quot;1-DataStream-API接口&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="Flink实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/categories/Flink-Action/"/>
    
    
      <category term="框架实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink实战全集</title>
    <link href="https://3pigsgu.github.io/info-tech-tutorial/2019/12/28/flink/flink-tutorial-00/"/>
    <id>https://3pigsgu.github.io/info-tech-tutorial/2019/12/28/flink/flink-tutorial-00/</id>
    <published>2019-12-28T05:21:46.000Z</published>
    <updated>2020-01-03T05:29:51.948Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Apache-Flink简介"><a href="#1-Apache-Flink简介" class="headerlink" title="1. Apache Flink简介"></a>1. Apache Flink简介</h2><h3 id="1-1-Flink的用途"><a href="#1-1-Flink的用途" class="headerlink" title="1.1. Flink的用途"></a>1.1. Flink的用途</h3><p>像Apache Hadoop和Apache Spark一样，Apache Flink是一个社区驱动的开源框架，用于分布式大数据分析。 Flink用Java编写，具有适用于Scala，Java和Python的API，可进行批处理和实时流分析。</p><h3 id="1-2-必备环境"><a href="#1-2-必备环境" class="headerlink" title="1.2. 必备环境"></a>1.2. 必备环境</h3><p>• 类似于UNIX的环境，例如Linux，Mac OS X或Cygwin(实际上也支持Windows环境)；<br>• Java 6.X或更高版本；<br>• [可选] Maven 3.0.4或更高版本。</p><h3 id="1-3-体系结构和技术栈"><a href="#1-3-体系结构和技术栈" class="headerlink" title="1.3. 体系结构和技术栈"></a>1.3. 体系结构和技术栈</h3><img src="/info-tech-tutorial/2019/12/28/flink/flink-tutorial-00/flinkstack.jpg" class="" title="Apache Flink Stack"><h4 id="1-3-1-Flink应用执行环境"><a href="#1-3-1-Flink应用执行环境" class="headerlink" title="1.3.1. Flink应用执行环境"></a>1.3.1. Flink应用执行环境</h4><p>Apache Flink是一个数据处理系统，是Hadoop MapReduce组件的替代产品。它带有自己的运行时，而不是在MapReduce之上构建。因此，它可以完全独立于Hadoop生态系统工作。<br>ExecutionEnvironment是执行程序的上下文。您可以根据需要使用不同的环境。</p><ul><li>JVM环境：Flink可以在单个Java虚拟机上运行，​​从而允许用户直接从其IDE测试和调试Flink程序。使用此环境时，您需要的只是正确的Maven依赖项。</li><li>本地环境：为了能够在正在运行的Flink实例上运行程序（而不是从IDE内部），您需要在计算机上安装Flink。</li><li>集群环境：以standalone集群或yarn集群的完全分布式方式运行Flink。</li></ul><h4 id="1-3-2-Flink-API接口"><a href="#1-3-2-Flink-API接口" class="headerlink" title="1.3.2. Flink API接口"></a>1.3.2. Flink API接口</h4><p>Flink可用于流或批处理。它们提供了三个API：</p><ul><li>DataStream API：流处理，即对无限制数据流的转换（过滤器，时间窗口，聚合）。 </li><li>DataSet API：批处理，即数据集的转换。</li><li>Table API：类似于SQL的表达语言（例如Spark中的数据框），可以嵌入批处理和流应用程序中。</li></ul><h4 id="1-3-3-构建层次"><a href="#1-3-3-构建层次" class="headerlink" title="1.3.3. 构建层次"></a>1.3.3. 构建层次</h4><p>在最基础的层级上，Flink由Sources，Transformations和Sinks组成。</p><img src="/info-tech-tutorial/2019/12/28/flink/flink-tutorial-00/building-blocks.png" class="" title="building blocks"><ul><li>Data Source：Flink处理的传入数据；</li><li>Transformations：Flink修改传入数据时的处理步骤；</li><li>Data Sink：Flink处理后在哪里发送数据。</li></ul><p>Sources和Sinks可以是本地/HDFS文件，数据库，消息队列等。已经有许多第三方连接器可用，或者您可以轻松创建自己的连接器。</p><h2 id="2-Flink实战章节"><a href="#2-Flink实战章节" class="headerlink" title="2. Flink实战章节"></a>2. Flink实战章节</h2><p>第一篇： <a href="/info-tech-tutorial/2019/12/28/flink-tutorial-01/" title="Apache Flink快速入门">Apache Flink快速入门</a><br>第二篇： <a href="/info-tech-tutorial/2019/12/30/flink-tutorial-02/" title="Apache Flink三类API接口">Apache Flink三类API接口</a></p><h2 id="Reference-List"><a href="#Reference-List" class="headerlink" title="Reference List"></a>Reference List</h2><ol><li><a href="https://blog.imaginea.com/apache-flink/" target="_blank" rel="noopener">Apache Flink</a>;</li><li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/" target="_blank" rel="noopener">Apache Flink Documentation</a>;</li><li><a href="https://riptutorial.com/apacheflink/topic/5798/getting-started-with-apache-flink" target="_blank" rel="noopener">Getting Started with Apache-Flink</a>.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Apache-Flink简介&quot;&gt;&lt;a href=&quot;#1-Apache-Flink简介&quot; class=&quot;headerlink&quot; title=&quot;1. Apache Flink简介&quot;&gt;&lt;/a&gt;1. Apache Flink简介&lt;/h2&gt;&lt;h3 id=&quot;1-1-Fli
      
    
    </summary>
    
    
      <category term="Flink实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/categories/Flink-Action/"/>
    
    
      <category term="框架实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink快速入门</title>
    <link href="https://3pigsgu.github.io/info-tech-tutorial/2019/12/28/flink-tutorial-01/"/>
    <id>https://3pigsgu.github.io/info-tech-tutorial/2019/12/28/flink-tutorial-01/</id>
    <published>2019-12-27T18:09:52.000Z</published>
    <updated>2020-01-02T15:11:19.262Z</updated>
    
    <content type="html"><![CDATA[<p>本篇的目的是带Apache Flink初学者引入Flink的大门，并提供一个Word Count的示例来了解如何使用Flink框架，希望能给各位开发爱好者提供浅显易懂的理解。</p><h2 id="1-Java运行环境"><a href="#1-Java运行环境" class="headerlink" title="1. Java运行环境"></a>1. Java运行环境</h2><p>Apache Flink是基于Java语言开发的，并且能运行在Windows, Linux和Mac OS操作系统上。为了能正常运行Flink, 唯一的前提条件是确保安装了Java 6或更高版本的版本，并且已设置JAVA_HOME环境变量。 </p><h2 id="2-Flink在Windows环境下运行"><a href="#2-Flink在Windows环境下运行" class="headerlink" title="2. Flink在Windows环境下运行"></a>2. Flink在Windows环境下运行</h2><p>如果要在Windows计算机上本地运行Flink，则需要下载Flink二进制发行版并解压缩。之后，您可以使用Windows批处理文件（.bat），也可以使用Cygwin运行Flink Jobmanager。 <a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">(Flink下载地址)</a></p><h3 id="2-1-以-bat文件启动"><a href="#2-1-以-bat文件启动" class="headerlink" title="2.1. 以.bat文件启动"></a>2.1. 以.bat文件启动</h3><p>要从Windows命令行启动Flink，请打开命令窗口，导航到Flink的bin/目录，然后运行start-cluster.bat。<br>注意：Java运行时环境的bin文件夹必须包含在Window的％PATH％变量中。</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> flink</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> bin</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-cluster.bat</span></span><br><span class="line">Starting a local cluster with one JobManager process and one TaskManager process.</span><br><span class="line">You can terminate the processes via CTRL-C in the spawned shell windows.</span><br><span class="line">Web interface by default on http://localhost:8081/.</span><br></pre></td></tr></table></figure><p>然后，您需要打开另一个终端使用flink.bat运行作业。</p><h3 id="2-2-安装Cygwin以Unix脚本启动"><a href="#2-2-安装Cygwin以Unix脚本启动" class="headerlink" title="2.2. 安装Cygwin以Unix脚本启动"></a>2.2. 安装Cygwin以Unix脚本启动</h3><p>使用Cygwin，您需要启动Cygwin终端，导航到Flink目录并运行start-cluster.sh脚本：</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> flink</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bin/start-cluster.sh</span></span><br><span class="line">Starting cluster.</span><br></pre></td></tr></table></figure><h2 id="3-Flink在非Windows环境下运行"><a href="#3-Flink在非Windows环境下运行" class="headerlink" title="3. Flink在非Windows环境下运行"></a>3. Flink在非Windows环境下运行</h2><ul><li>在此处<a href="https://flink.apache.org/downloads.html" target="_blank" rel="noopener">下载</a>最新的flink二进制文件, 可以选择任意的scala版本：例如Apache Flink 1.9.1 for Scala 2.12。如果您打算使用Hadoop，请选择hadoop相关构件版本。</li><li>解压文件压缩包并启动Flink：<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar xzvf flink-1.9.1-bin-scala_2.12.tar.gz   #Unpack the downloaded archive</span><br><span class="line">./flink/bin/start-cluster.sh    #Start Flink</span><br></pre></td></tr></table></figure>Flink已配置为在本地运行。要确保flink正在运行，您可以检查flink/log/中的日志，或打开在http：//localhost：8081上运行的flink jobManager的界面。<img src="/info-tech-tutorial/2019/12/28/flink-tutorial-01/jobmanager-1.png" class=""></li><li>停止Flink：<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./flink/bin/stop-cluster.sh</span><br></pre></td></tr></table></figure></li></ul><h2 id="4-Flink开发环境"><a href="#4-Flink开发环境" class="headerlink" title="4. Flink开发环境"></a>4. Flink开发环境</h2><p>要从您的IDE运行flink程序（我们可以使用Eclipse或Intellij IDEA（推荐）），您需要两个依赖项：flink-java/flink-scala和flink-clients（自2016年2月起）。可以使用Maven和SBT添加这些JARS（如果使用的是Scala）。这里我们只介绍如何使用Java进行获取依赖项，而且后面的代码也同样基于Java程序作为示例。<br>核心的Maven依赖包括如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;properties&gt;</span><br><span class="line">        &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt;</span><br><span class="line">        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;&#x2F;project.reporting.outputEncoding&gt;</span><br><span class="line">        &lt;java.version&gt;1.8&lt;&#x2F;java.version&gt;</span><br><span class="line">        &lt;flink.version&gt;1.9.1&lt;&#x2F;flink.version&gt;</span><br><span class="line">        ...</span><br><span class="line">&lt;&#x2F;properties&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-streaming-java_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-clients_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-wikiedits_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="5-Socket-Window-Word-Count示例代码"><a href="#5-Socket-Window-Word-Count示例代码" class="headerlink" title="5. Socket Window Word Count示例代码"></a>5. Socket Window Word Count示例代码</h2><p>您可以在GitHub上找到此SocketWindowWordCount示例的完整<a href="">Java版源代码</a>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.ReduceFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketWindowWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// the port to connect to</span></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> port;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">            port = params.getInt(<span class="string">"port"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.err.println(<span class="string">"No port specified. Please run 'SocketWindowWordCount --port &lt;port&gt;'"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// get the execution environment</span></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// get input data by connecting to the socket</span></span><br><span class="line">        DataStream&lt;String&gt; text = env.socketTextStream(<span class="string">"localhost"</span>, port, <span class="string">"\n"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// parse the data, group it, window it, and aggregate the counts</span></span><br><span class="line">        DataStream&lt;WordWithCount&gt; windowCounts = text</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, WordWithCount&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;WordWithCount&gt; out)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">for</span> (String word : value.split(<span class="string">"\\s"</span>)) &#123;</span><br><span class="line">                            out.collect(<span class="keyword">new</span> WordWithCount(word, <span class="number">1L</span>));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(<span class="string">"word"</span>)</span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">5</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">                .reduce(<span class="keyword">new</span> ReduceFunction&lt;WordWithCount&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> WordWithCount <span class="title">reduce</span><span class="params">(WordWithCount a, WordWithCount b)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> WordWithCount(a.word, a.count + b.count);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// print the results with a single thread, rather than in parallel</span></span><br><span class="line">        windowCounts.print().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Socket Window WordCount"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Data type for words with count</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordWithCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> String word;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">long</span> count;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">WordWithCount</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">WordWithCount</span><span class="params">(String word, <span class="keyword">long</span> count)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.word = word;</span><br><span class="line">            <span class="keyword">this</span>.count = count;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> word + <span class="string">" : "</span> + count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>更多参考：<a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/socket/SocketWindowWordCount.java" target="_blank" rel="noopener">Flink官方示例代码</a></p><h2 id="5-运行Socket-Window-Word-Count示例"><a href="#5-运行Socket-Window-Word-Count示例" class="headerlink" title="5. 运行Socket Window Word Count示例"></a>5. 运行Socket Window Word Count示例</h2><p>接下来，我们来运行演示名为SocketWindowWordCount的Flink应用示例。</p><ul><li>首先，我们使用netcat在CMD命令窗口中通过以下方式启动本地服务：<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc -l -p 9000</span></span><br></pre></td></tr></table></figure></li><li>然后，通过flink run命令提交Flink程序：<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/flink run SocketWindowWordCount.jar --port 9000</span></span><br><span class="line">Starting execution of program</span><br></pre></td></tr></table></figure>程序连接到socket套接字并等待输入。您可以检查Web界面以验证作业是否按预期运行：<img src="/info-tech-tutorial/2019/12/28/flink-tutorial-01/jobmanager-2.png" class=""><img src="/info-tech-tutorial/2019/12/28/flink-tutorial-01/jobmanager-3.png" class=""></li><li>接着我在CMD命令窗口输入下面数个单词：<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc -l -p 9000</span></span><br><span class="line">lorem ipsum</span><br><span class="line">ipsum ipsum ipsum</span><br><span class="line">bye</span><br></pre></td></tr></table></figure></li><li>我们会在启动的Flink集群的任一个窗口看到如下输出：<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lorem : 1</span><br><span class="line">bye : 1</span><br><span class="line">ipsum : 4</span><br></pre></td></tr></table></figure>这里如果遇到应用程序报Flink Connection refused错误，请参考如下链接：</li></ul><ul><li><a href="http://mail-archives.apache.org/mod_mbox/flink-user/201808.mbox/%3CCAPOsGyZe1=VYO-Kt9iKEg3QrnNoZp6v8yFErj=KU+MxA5hrMJw@mail.gmail.com%3E" target="_blank" rel="noopener">connection failed when running flink in a cluster</a></li><li><a href="https://stackoverflow.com/questions/53784757/unable-to-run-flink-example-program-connection-refused" target="_blank" rel="noopener">Unable to run flink example program,Connection refused</a></li><li><a href="https://stackoverflow.com/questions/52595826/flink-connection-refused-localhost-127-0-0-18081" target="_blank" rel="noopener">Flink Connection refused: localhost/127.0.0.1:8081</a></li></ul><h2 id="References："><a href="#References：" class="headerlink" title="References："></a>References：</h2><ol><li><a href="https://ci.apache.org/projects/flink/flink-docs-stable/" target="_blank" rel="noopener">Apache Flink Documentation</a>;</li><li><a href="https://riptutorial.com/apacheflink/topic/5798/getting-started-with-apache-flink" target="_blank" rel="noopener">Getting Started with Apache-Flink</a>.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇的目的是带Apache Flink初学者引入Flink的大门，并提供一个Word Count的示例来了解如何使用Flink框架，希望能给各位开发爱好者提供浅显易懂的理解。&lt;/p&gt;
&lt;h2 id=&quot;1-Java运行环境&quot;&gt;&lt;a href=&quot;#1-Java运行环境&quot; cla
      
    
    </summary>
    
    
      <category term="Flink实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/categories/Flink-Action/"/>
    
    
      <category term="框架实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow安装详解与样例Demo演示</title>
    <link href="https://3pigsgu.github.io/info-tech-tutorial/2019/12/26/tensorflow/tensorflow-tutorial-01/"/>
    <id>https://3pigsgu.github.io/info-tech-tutorial/2019/12/26/tensorflow/tensorflow-tutorial-01/</id>
    <published>2019-12-26T03:40:37.000Z</published>
    <updated>2020-01-02T15:11:39.216Z</updated>
    
    <content type="html"><![CDATA[<p>在使用Tensorflow进行机器学习之前， 我们需要在自己的机器上先安装相关的软件。我们可以安照<a href="https://tensorflow.google.cn/install" target="_blank" rel="noopener">Tensorflow</a>官方网站上提供的步骤，一步一步的进行安装。官网上会提供多种环境下的部署方法，包括：Linux， Windows，MacOS和Docker容器等等。一般建议在容器或者Virtualenv这样的虚拟环境下进行Tensorflow的安装以及与此配套的一些第三方软件的集成。比较好的一种方式是对于生产环境可以采用<a href="https://www.docker.com/" target="_blank" rel="noopener">Docker</a>这类的容器进行Tensorflow的部署，而对于开发环境可以采用<a href="https://docs.conda.io" target="_blank" rel="noopener">Conda</a>的依赖环境进行安装。 这样可以保证不同版本之间的强依赖关系，避免依赖导致的不必要的冲突发生。</p><p>这篇文章将一步一步的带大家在Conda的依赖环境下安装Tensorflow和其周边的第三方插件， 并且演示一个使用Tensorflow的小示例。接下来，我在Windows设备上操作整个过程。</p><h2 id="1-Minconda安装"><a href="#1-Minconda安装" class="headerlink" title="1. Minconda安装"></a>1. Minconda安装</h2><p>Minconda和Conda的区别在于Minconda其实就是Conda的一个子集，是Conda的一个简化安装版：</p><p>如果您选择Anaconda，可能：<br>是conda或Python的新手，喜欢一次性安装Python和150多个科学软件包。<br>有时间和磁盘空间-可能需要几分钟和300MB的磁盘空间。<br>不想单独安装要使用的每个软件包。</p><p>如果您选择Miniconda，可能：<br>不介意安装要单独使用的每个软件包。<br>没有时间或磁盘空间来一次安装150个以上的软件包。<br>希望快速访问Python和conda命令，并且希望以后再整理其他程序。</p><p>这里提供Miniconda安装软件的<a href="https://docs.conda.io/en/latest/miniconda.html#other-resources" target="_blank" rel="noopener">下载地址</a>, 请依据自己的喜好和机器环境酌情选择。如果你下直接安装完整的Conda软件，也可以在这个链接<a href="https://www.anaconda.com/distribution/" target="_blank" rel="noopener">下载</a>。<br>详细的操作文档请参见：<a href="https://docs.conda.io/projects/conda/en/latest/index.html" target="_blank" rel="noopener">Conda文档</a>。</p><h3 id="1-1-管理Conda"><a href="#1-1-管理Conda" class="headerlink" title="1.1. 管理Conda"></a>1.1. 管理Conda</h3><ul><li><p>通查看Conda的版本验证Conda是否安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\44355&gt; conda --version</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\44355&gt; conda -V</span><br></pre></td></tr></table></figure><p>Conda会返回版本号，例如：conda 4.8.0。</p></li><li><p>查看更详细的Conda信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\44355&gt; conda info</span><br></pre></td></tr></table></figure><p>Conda返回更多的安装信息，包括安装路径、Conda版本、Python版本、基础环境路径、包缓存路径等。</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">    active environment : base</span><br><span class="line">   active env location : D:\software\Miniconda3</span><br><span class="line">           shell level : 1</span><br><span class="line">      user config file : C:\Users\44355\.condarc</span><br><span class="line">populated config files : C:\Users\44355\.condarc</span><br><span class="line">         conda version : 4.8.0</span><br><span class="line">   conda-build version : not installed</span><br><span class="line">        python version : 3.7.3.final.0</span><br><span class="line">      virtual packages :</span><br><span class="line">      base environment : D:\software\Miniconda3  (writable)</span><br><span class="line">          channel URLs : https://repo.anaconda.com/pkgs/main/win-64</span><br><span class="line">                         https://repo.anaconda.com/pkgs/main/noarch</span><br><span class="line">                         https://repo.anaconda.com/pkgs/r/win-64</span><br><span class="line">                         https://repo.anaconda.com/pkgs/r/noarch</span><br><span class="line">                         https://repo.anaconda.com/pkgs/msys2/win-64</span><br><span class="line">                         https://repo.anaconda.com/pkgs/msys2/noarch</span><br><span class="line">         package cache : D:\software\Miniconda3\pkgs</span><br><span class="line">                         C:\Users\44355\.conda\pkgs</span><br><span class="line">                         C:\Users\44355\AppData\Local\conda\conda\pkgs</span><br><span class="line">      envs directories : D:\software\Miniconda3\envs</span><br><span class="line">                         C:\Users\44355\.conda\envs</span><br><span class="line">                         C:\Users\44355\AppData\Local\conda\conda\envs</span><br><span class="line">              platform : win-64</span><br><span class="line">            user-agent : conda/4.8.0 requests/2.22.0 CPython/3.7.3 Windows/10 Windows/10.0.17763</span><br><span class="line">         administrator : False</span><br><span class="line">            netrc file : None</span><br><span class="line">          offline mode : False</span><br></pre></td></tr></table></figure></li><li><p>更新Conda到最新版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\44355&gt; conda update conda</span><br></pre></td></tr></table></figure><p>更多参考：<a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-conda.html" target="_blank" rel="noopener">Conda Management</a></p></li></ul><h3 id="1-2-Conda的环境管理"><a href="#1-2-Conda的环境管理" class="headerlink" title="1.2. Conda的环境管理"></a>1.2. Conda的环境管理</h3><ul><li><p>命令行创建一个新环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\44355&gt; conda create --name tensorflow-env</span><br></pre></td></tr></table></figure></li><li><p>命令行激活指定环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\44355&gt; conda activate tensorflow-env</span><br></pre></td></tr></table></figure><p>激活环境后所有的操作会在当前激活的环境下进行。</p></li><li><p>命令行停用当前环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt; conda deactivate</span><br></pre></td></tr></table></figure><p>更多参考：<a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html" target="_blank" rel="noopener">Conda Environment Management</a></p></li></ul><h2 id="2-在tensorflow-env环境下进行Python安装"><a href="#2-在tensorflow-env环境下进行Python安装" class="headerlink" title="2. 在tensorflow-env环境下进行Python安装"></a>2. 在tensorflow-env环境下进行Python安装</h2><ul><li>当我们在Conda中激活tensorflow-env环境，则出现如下提示：<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt;</span><br></pre></td></tr></table></figure>此时表明当前已经进入tensorflow-env环境。</li></ul><ul><li>由于在安装Conda的时候其自身内嵌安装了相应的Python版本，因此我们无需再额外安装Python,就可以直接在tensorflow-env环境中安装Tensorflow。<br>首先查看Tensorflow各个版本：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt; conda search tensorflow</span><br></pre></td></tr></table></figure>或者<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt; conda search tensorflow-gpu</span><br></pre></td></tr></table></figure>选择一个安装版本进行安装：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt; conda install tensorflow=1.8.0</span><br></pre></td></tr></table></figure>或者直接安装最新版本的Tensorflow：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt; conda install tensorflow</span><br></pre></td></tr></table></figure></li></ul><ul><li>安装完成后验证Tensorflow是否安装成功：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt; python</span><br><span class="line">&gt;&gt;&gt; import tensorflow as tf  <span class="comment">#不报错即安装成功</span></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">exit</span>()</span><br><span class="line">(tensorflow-env) C:\Users\44355&gt; conda deactivate</span><br></pre></td></tr></table></figure>更多参考：<a href="https://tensorflow.google.cn/install" target="_blank" rel="noopener">Tensorflow官方安装</a></li></ul><h2 id="3-在tensorflow-env环境下安装常用的第三方软件"><a href="#3-在tensorflow-env环境下安装常用的第三方软件" class="headerlink" title="3. 在tensorflow-env环境下安装常用的第三方软件"></a>3. 在tensorflow-env环境下安装常用的第三方软件</h2><h3 id="3-1-安装Jupyter"><a href="#3-1-安装Jupyter" class="headerlink" title="3.1. 安装Jupyter"></a>3.1. 安装Jupyter</h3><ul><li><p>Jupyter是一款为了开发跨多种编程语言的交互式计算的开源软件，它提供开放的标准和服务。<br>JupyterLab是用于Jupyter笔记本，代码和数据的基于Web的交互式开发环境。 JupyterLab非常灵活：配置和安排用户界面以支持数据科学，科学计算和机器学习中的各种工作流程。 JupyterLab是可扩展的和模块化的：编写可添加新组件并与现有组件集成的插件。接下来我们通过pip命令来安装JupyterLab。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt; pip install jupyterlab</span><br></pre></td></tr></table></figure><p>更多参考：<a href="https://jupyter.org/install" target="_blank" rel="noopener">Jupyter官方安装</a></p></li><li><p>Jupyter Notebook是Jupyter中的一个开源Web应用程序，允许您创建和共享包含实时代码，方程式，可视化效果和叙述文本的文档。用途包括：数据清理和转换，数值模拟，统计建模，数据可视化，机器学习等。我们可以通过命令来启动Jupyter Notebook服务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt; jupyter notebook</span><br></pre></td></tr></table></figure><p>终端上会打印如下启动信息：</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[I 10:20:26.155 NotebookApp] Serving notebooks from local directory: C:\Users\44355</span><br><span class="line">[I 10:20:26.160 NotebookApp] The Jupyter Notebook is running at:</span><br><span class="line">[I 10:20:26.161 NotebookApp] http://localhost:8888/?token=00f06b6e31b020740277e2d20eef416da634cc55d6f6a2ba</span><br><span class="line">[I 10:20:26.162 NotebookApp]  or http://127.0.0.1:8888/?token=00f06b6e31b020740277e2d20eef416da634cc55d6f6a2ba</span><br><span class="line">[I 10:20:26.162 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span><br><span class="line">[C 10:20:26.406 NotebookApp]</span><br><span class="line"></span><br><span class="line">    To access the notebook, open this file in a browser:</span><br><span class="line">        file:///C:/Users/44355/AppData/Roaming/jupyter/runtime/nbserver-138920-open.html</span><br><span class="line">    Or copy and paste one of these URLs:</span><br><span class="line">        http://localhost:8888/?token=00f06b6e31b020740277e2d20eef416da634cc55d6f6a2ba</span><br><span class="line">     or http://127.0.0.1:8888/?token=00f06b6e31b020740277e2d20eef416da634cc55d6f6a2ba</span><br></pre></td></tr></table></figure><p>接下来会自动打开浏览器进入Jupyter Notebook操作界面：</p><img src="/info-tech-tutorial/2019/12/26/tensorflow/tensorflow-tutorial-01/Jupyter_Notebook_Interface.png" class="" title="Jupyter Notebook操作界面"><p>更多参考：<a href="https://jupyter.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Jupyter官方文档</a></p></li></ul><h3 id="3-2-安装Matplotlib"><a href="#3-2-安装Matplotlib" class="headerlink" title="3.2. 安装Matplotlib"></a>3.2. 安装Matplotlib</h3><p>Matplotlib是一个Python 2D绘图库，它以多种硬拷贝格式和跨平台的交互式环境生成出版物质量的图形。 Matplotlib可用于Python脚本，Python和IPython Shell，Jupyter Notebook，Web应用程序服务器和四个图形用户界面工具包。<br>接下来我们通过Python来安装Matplotlib插件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow-env) C:\Users\44355&gt; python -m pip install -U pip</span><br><span class="line">(tensorflow-env) C:\Users\44355&gt; python -m pip install -U matplotlib</span><br></pre></td></tr></table></figure><p>更多参考：<a href="https://matplotlib.org/users/installing.html" target="_blank" rel="noopener">Matplotlib官方安装</a><br>现在我们只需几行代码就可以生成图表，直方图，功率谱，条形图，误差图，散点图等。</p><h2 id="4-示例代码演示TensorFlow-Jupyter-Notebook-and-Matplotlib的使用"><a href="#4-示例代码演示TensorFlow-Jupyter-Notebook-and-Matplotlib的使用" class="headerlink" title="4. 示例代码演示TensorFlow, Jupyter Notebook, and Matplotlib的使用"></a>4. 示例代码演示TensorFlow, Jupyter Notebook, and Matplotlib的使用</h2><p>接下来，我们在Jupyter Notebook界面中写一段很简单的演示代码。<br>首先我们在Jupyter Notebook操作界面的右上角单击<font style=" border:1px solid; -moz-border-radius:5px; -webkit-border-radius:5px; border-radius:5px; background-color: #F8F8F8;"> new </font>按钮，然后选择Python版本，这边选择的是安装Conda时自带的Python3.</p><img src="/info-tech-tutorial/2019/12/26/tensorflow/tensorflow-tutorial-01/Jupyter_Edit_1.png" class=""><p>进入代码操作界面后，可以把默认代码文件名Untitled修改成自己定义的名字，这里修改成“My First Notebook”。</p><img src="/info-tech-tutorial/2019/12/26/tensorflow/tensorflow-tutorial-01/Jupyter_Edit_2.png" class=""><p>然后我们在代码块区域输入如下示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">a = tf.random_normal([<span class="number">2</span>,<span class="number">20</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">out = sess.run(a)</span><br><span class="line">x, y = out</span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>代码中tf.random_normal()函数用于从服从指定正太分布的数值中取出指定个数的值。 shape=[2,20]是输出张量的形状，一个2行20列的矩阵。然后tensorflow运行计算后的输出按行分别传给x和y两个变量。最后以Matplotlib的scatter散点图的方式图形化展示，其中x代表横轴值，y代表纵轴值。<br>代码可以分成多个代码块，代码块可以通过左上角<font style=" border:1px solid; -moz-border-radius:5px; -webkit-border-radius:5px; border-radius:5px; background-color: #F8F8F8;"> + </font>号增加，最终完成界面如下：</p><img src="/info-tech-tutorial/2019/12/26/tensorflow/tensorflow-tutorial-01/Jupyter_Edit_3.png" class=""><p>完成Python示例代码后，可以点击<font style=" border:1px solid; -moz-border-radius:5px; -webkit-border-radius:5px; border-radius:5px; background-color: #F8F8F8;"> Run </font>按钮逐步执行代码块，这里代码调用tensorflow正太分布算法，最终结果会使用Matplotlib去直观的展示20个点的随机正太分布情况。</p><img src="/info-tech-tutorial/2019/12/26/tensorflow/tensorflow-tutorial-01/Jupyter_Edit_4.png" class=""><h2 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h2><p>至此整个Tensorflow的安装详解和示例代码演示到此结束，如果需要了解更多的相关细节，请阅读本文中提供的更多的官方链接。如有疑问欢迎大家留言！欢迎来踩留印！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在使用Tensorflow进行机器学习之前， 我们需要在自己的机器上先安装相关的软件。我们可以安照&lt;a href=&quot;https://tensorflow.google.cn/install&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Tensorflow
      
    
    </summary>
    
    
      <category term="Tensorflow实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/categories/Tensorflow-Action/"/>
    
    
      <category term="框架实战" scheme="https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"/>
    
  </entry>
  
</feed>
