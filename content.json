{"meta":{"title":"Infomatic Technique Tutorial","subtitle":"Jack Gu's personal tech-tutorial","description":"This is my personal blog to share latest techniques and theories in High-Tech field.","author":"Jack Gu","url":"https://3pigsgu.github.io/info-tech-tutorial","root":"/info-tech-tutorial/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2019-12-22T14:45:07.436Z","updated":"2019-12-22T14:45:07.436Z","comments":false,"path":"/404.html","permalink":"https://3pigsgu.github.io/info-tech-tutorial/404.html","excerpt":"","text":""},{"title":"关于","date":"2019-12-17T08:46:50.000Z","updated":"2019-12-18T10:02:03.930Z","comments":false,"path":"about/index.html","permalink":"https://3pigsgu.github.io/info-tech-tutorial/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-12-17T08:47:01.000Z","updated":"2019-12-18T06:04:12.808Z","comments":true,"path":"categories/index.html","permalink":"https://3pigsgu.github.io/info-tech-tutorial/categories/index.html","excerpt":"","text":""},{"title":"项目","date":"2019-12-17T08:47:46.000Z","updated":"2019-12-18T10:00:37.308Z","comments":false,"path":"repository/index.html","permalink":"https://3pigsgu.github.io/info-tech-tutorial/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-12-17T08:41:10.000Z","updated":"2019-12-18T05:12:16.375Z","comments":true,"path":"tags/index.html","permalink":"https://3pigsgu.github.io/info-tech-tutorial/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"使用Bypy插件实现Linux系统中百度网盘的上传下载操作","slug":"other/linux_command_for_bypy","date":"2020-02-12T06:32:12.000Z","updated":"2020-02-12T09:34:56.844Z","comments":true,"path":"2020/02/12/other/linux_command_for_bypy/","link":"","permalink":"https://3pigsgu.github.io/info-tech-tutorial/2020/02/12/other/linux_command_for_bypy/","excerpt":"","text":"对于很多开发人员来说经常会面临大量数据需要备份的问题，比如数据库的bin-log数据日志，亦或软件包。而往往较大容量的备份数据从服务器上Download需要花费很长的数据，并且使用的Xftp文件传输工具也不可能持续稳定的长连接传输文件，另外连接中断后也并不能支持断点续传重试。因此，我们会想是否有替代的方式解决上面提到的问题。这篇文章则利用百度网盘来实现这个牛掰的备份功能。 大多数使用过百度网盘的人想必都有其免费提供的2TB存储空间。一般来讲，这个免费的存储容量足够用于个人的数据备份。那么如果在linux系统中把数据备份到百度网盘上，下面我们具体讲解百度云的一款Python客户端工具Bypy。 1. Centos依赖安装在安装Bypy软件工具之前，这里需要稍微提一下一些注意点。对于Centos 7操作系统在安装pip之前，需要先如下安装依赖： 1# yum -y install epel-release 2. 安装软件工具123# yum -y install python-pip# pip install requests# pip install bypy 3. 授权登陆执行 bypy info，显示下边信息，根据提示，通过浏览器访问下边灰色的https链接，如果此时百度网盘账号正在登陆，会出现长串授权码，复制。 123456# bypy infoPlease visit: # 访问下边这个连接，复制授权码https:&#x2F;&#x2F;openapi.baidu.com&#x2F;oauth&#x2F;2.0&#x2F;authorize?scope&#x3D;basic+netdisk&amp;redirect_uri&#x3D;oob&amp;response_type&#x3D;code&amp;client_id&#x3D;q8WE4EpCsau1oS0MplgMKNBnAnd authorize this appPaste the Authorization Code here within 10 minutes.Press [Enter] when you are done # 提示在下边粘贴授权码 在下边图示红色位置粘贴授权码，耐心等待一会即可(1-2分钟) 12345678Press [Enter] when you are donea288f3d775fa905a6911692a0808f6a8Authorizing, please be patient, it may take upto None seconds...Authorizing&#x2F;refreshing with the OpenShift server ...OpenShift server failed, authorizing&#x2F;refreshing with the Heroku server ...Successfully authorizedQuota: 2.015TBUsed: 740.493GB 授权成功。 4. 上传文件到云盘由于百度PCS API权限限制，程序只能存取百度云端/apps/bypy目录下面的文件和目录。我们可以通过： 12# bypy list&#x2F;apps&#x2F;bypy ($t $f $s $m $d): 把本地当前目录下的文件同步到百度云盘： 1# bypy upload &#x2F;文件路径&#x2F;文件名 &#x2F;网盘路径 把云盘上的内容同步到本地: 1# bypy downdir 比较本地当前目录和云盘根目录，看是否一致，来判断是否同步成功： 1# bypy compare 把当前目录同步到云盘 1# bypy syncup PS: 运行时添加-v参数，会显示进度详情。","categories":[{"name":"其他","slug":"Other","permalink":"https://3pigsgu.github.io/info-tech-tutorial/categories/Other/"}],"tags":[{"name":"其他","slug":"Other","permalink":"https://3pigsgu.github.io/info-tech-tutorial/tags/Other/"}]},{"title":"Apache Spark Standalone集群模式部署","slug":"spark-tutorial-01","date":"2020-01-14T09:20:19.000Z","updated":"2020-01-21T07:57:48.311Z","comments":true,"path":"2020/01/14/spark-tutorial-01/","link":"","permalink":"https://3pigsgu.github.io/info-tech-tutorial/2020/01/14/spark-tutorial-01/","excerpt":"","text":"1. 环境准备为了部署Apache Spark Standalone集群模式，本次准备了三台Linux虚拟机来实现集群：一台Master和两台Slaves。Master: 192.168.71.130Slave: 192.168.71.131Slave: 192.168.71.132为了保证三台虚拟机之间能正常访问通讯，需要在每台虚拟机上配置/etc目录下的hosts和hostname文件。所有的hosts文件设置如下： 123hadoop0 192.168.71.130hadoop1 192.168.71.131hadoop2 192.168.71.132 各自对应IP的虚拟机上的hostname设置成上面相对应的IP地址前面的别名，例如IP为192.168.71.130的虚拟机的hostname设置成hadoop0。然后，通过ping命令测试各节点之间是否互通。如果各节点之间有使用密钥验证登录，那么需要额外的生成密钥并添加到各节点认证文件中，以实现各节点的免密登录。生成密钥： 1ssh-keygen 一路默认回车，这样在当前用户的主目录下会生成.ssh文件夹。进入这个文件夹，首先把本机的公钥加入认证文件: 1cat id_rsa.pub &gt;&gt; authorized_keys 2. 下载Spark接下来，我们去Apache Spark官网下载相对应的Spark Pack。这里我们选择2.4.4 release版本并预编译包含Hadoop2.7的lib库的Spark。(Spark下载地址) 这里我们选择包含hadoop的lib库的Spark是为了免去配置Spark所需的hadoop依赖库的繁琐过程，具体的不带hadoop lib库的Spark的手动配置过程，会在Spark与Hadoop的整合应用这一章再展开。然后点击下载链接，进入跳转页面后，任意选择一个下载链接。接着在预先准备的Linux虚拟机上下载Spark Package, 例如： 12wget http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;apache&#x2F;spark&#x2F;spark-2.4.4&#x2F;spark-2.4.4-bin-hadoop2.7.tgztar zxvf spark-2.4.4-bin-hadoop2.7.tgz 在/etc/profile中添加SPARK_HOME路径： 123456vi &#x2F;etc&#x2F;profileexport SPARK_HOME&#x3D;&#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7export JAVA_HOME&#x3D;&#x2F;home&#x2F;guty&#x2F;software&#x2F;jdk1.8.0_221export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$SPARK_HOME&#x2F;bin:$PATHsource &#x2F;etc&#x2F;profile 3. 配置Spark Master在准备好Spark Package之后，我们上Master节点（192.168.71.130），进行相应的配置。首先我们进入Spark目录spark-2.4.4-bin-hadoop2.7，然后配置spark环境变量,修改conf/spark-env.sh文件,在文件末尾添加JAVA_HOME路径（指定所使用的JDK）。 123cp conf&#x2F;spark-env.sh.template conf&#x2F;spark-env.shvi conf&#x2F;spark_env.shexport JAVA_HOME&#x3D;&#x2F;home&#x2F;guty&#x2F;software&#x2F;jdk1.8.0_221 接着配置slave机器的信息,修改conf/slaves文件,在文件末尾添加集群各台主机的hostname： 12345cp conf&#x2F;slaves.template conf&#x2F;slavesvi conf&#x2F;slaveshadoop0hadoop1hadoop2 4. 配置Spark Workers (Slaves)当Spark Master配置完成后，配置各个Spark Workers相对比较简单。这里使用linux的scp命令直接在各主机节点的局域网内床送复制SparK Pack程序文件。例如在节点192.168.71.131和192.168.71.132上执行如下命令： 1scp root@192.168.71.130:&#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7 &#x2F;home&#x2F;guty&#x2F;software 同样，在节点192.168.71.131和192.168.71.132上/etc/profile文件中添加SPARK_HOME路径。 5. 启动和停止Spark 启动Spark启动后会显示如下信息： 123456.&#x2F;sbin&#x2F;start-all.shstarting org.apache.spark.deploy.master.Master, logging to &#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.master.Master-1-hadoop0.outhadoop2: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-hadoop2.outhadoop1: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-hadoop1.outhadoop0: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;home&#x2F;guty&#x2F;software&#x2F;spark-2.4.4-bin-hadoop2.7&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-hadoop0.out 然后可以访问Spark Web UI界面，http://192.168.71.130:8080是Spark Master节点web界面，下图可以看到各个workers节点的列表信息： 单击上图其中某一个worker节点，可以查看该worker节点的任务执行情况，如下图： 停止Spark停止Spark会显示如下信息： 123456.&#x2F;sbin&#x2F;stop-all.sh hadoop1: stopping org.apache.spark.deploy.worker.Workerhadoop0: stopping org.apache.spark.deploy.worker.Workerhadoop2: stopping org.apache.spark.deploy.worker.Workerstopping org.apache.spark.deploy.master.Master References Spark Standalone Mode Spark的独立集群模式部署","categories":[{"name":"Spark实战","slug":"Spark-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/categories/Spark-Action/"}],"tags":[{"name":"框架实战","slug":"Architecture-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"}]},{"title":"Apache Spark实战全集","slug":"spark/spark-tutorial-00","date":"2020-01-14T03:21:24.000Z","updated":"2020-01-14T09:22:42.960Z","comments":true,"path":"2020/01/14/spark/spark-tutorial-00/","link":"","permalink":"https://3pigsgu.github.io/info-tech-tutorial/2020/01/14/spark/spark-tutorial-00/","excerpt":"","text":"1. Apache Spark简介Apache Spark是一款轻量级快速统一分析引擎。作为一个开源的大数据处理框架，Spark围绕速度，易用性和复杂分析能力而构建。开发人员在处理大量数据或处理内存有限和时间过长时通常会使用Spark。 1.1. 超快的速度Apache Spark在批量数据和流量数据处理方面都实现了高性能，并提供了优异的调度器、查询器和物理执行引擎给大家使用。例如下图，Spark比Hadoop运行工作负载的速度提高了100倍。 1.2. 简单易用Spark提供了80多个高级操作器，可轻松构建并行应用程序。您可以使用Scala，Python，R和SQL Shell与Spark进行交互。 12df = spark.read.json(\"logs.json\") df.where(\"age &gt; 21\") .select(\"name.first\").show() 1.3. 通用性Spark可以结合SQL，流和复杂的分析一起使用。Spark为包括SQL和DataFrames，用于机器学习的MLlib，GraphX和Spark Streaming在内的库提供了强大的支持。您可以在同一应用程序中无缝组合这些库。 1.4. 随处运行Spark可在Hadoop，Apache Mesos，Kubernetes，独立或云中运行。它可以访问各种数据源。您可以在EC2，Hadoop YARN，Mesos或Kubernetes上使用独立集群模式运行Spark。访问HDFS，Alluxio，Apache Cassandra，Apache HBase，Apache Hive和数百种其他数据源中的数据。 2. Spark实战章节第一篇： Apache Spark Standalone集群模式部署 Reference List Apache Spark官网 Apache Spark Documentation","categories":[{"name":"Spark实战","slug":"Spark-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/categories/Spark-Action/"}],"tags":[{"name":"框架实战","slug":"Architecture-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"}]},{"title":"Prometheus+Grafana监控平台搭建与配置","slug":"other/prometheus_grafana_monitoring","date":"2020-01-07T07:19:02.000Z","updated":"2020-01-07T15:35:28.831Z","comments":true,"path":"2020/01/07/other/prometheus_grafana_monitoring/","link":"","permalink":"https://3pigsgu.github.io/info-tech-tutorial/2020/01/07/other/prometheus_grafana_monitoring/","excerpt":"","text":"1. Prometheus简介Prometheus是最初在SoundCloud上构建的开源系统监视和警报工具包。自2012年成立以来，许多公司和组织都采用了Prometheus，该项目拥有非常活跃的开发人员和用户社区。现在，它是一个独立的开源项目，并且独立于任何公司进行维护。为了强调这一点并阐明项目的治理结构，Prometheus于2016年加入了Cloud Native Computing Foundation，这是继Kubernetes之后的第二个托管项目。 1.1. Prometheus特征普罗米修斯的主要特点是： 一个多维数据模型，其中包含通过度量标准名称和键/值对标识的时间序列数据； 具备PromQL查询语言，是一种灵活的可利用维度的查询语言； 不依赖分布式存储；单服务器节点自治； 通过HTTP上的拉取模型数据进行时间序列收集； 通过中间网关支持推送时间序列； 通过服务发现或静态配置发现目标； 多种图形和仪表板支持模式。 1.2. Prometheus组件构成普罗米修斯的生态系统是由各种不同的组件构成，其中组件都是可选的： Prometheus Server主服务器，它会抓取并存储时间序列数据； Client Library客户端库提供了可检索的应用代码； Push gateway推送网关支持短生命周期的Job工作任务； 特定的Exporters提供了多种HAProxy， StatsD，Graphite等定制服务； Alertmanager提供了告警的控制； 并兼容多种辅助工具。 1.3. Prometheus体系架构下图说明了Prometheus的体系结构及其某些生态系统组件： Prometheus直接或通过中间推送网关从已检测作业中删除指标，以用于短期作业。它在本地存储所有报废的样本，并对这些数据运行规则，以汇总和记录现有数据中的新时间序列，或生成警报。 Grafana或其他API使用者可用于可视化收集的数据。 2. 安装配置Prometheus下面介绍如何安装Prometheus，并且利用两个exporter去监控Linux server和Mysql数据库。这里需要用到node_exporter和mysqld_exporter两个收集器。 首先我们下载Prometheus Server，并使用prometheus.yml配置文件启动服务:1234$ wget https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;releases&#x2F;download&#x2F;v2.15.2&#x2F;prometheus-2.15.2.linux-amd64.tar.gz$ tar zxvf prometheus-2.15.2.linux-amd64.tar.gz$ cd prometheus-2.15.2.linux-amd64$ .&#x2F;prometheus --config.file&#x3D;prometheus.yml Prometheus本身提供web界面。当Prometheus Server启动起来后，可通过浏览器访问地址http://localhost:9090/graph 来进入Prometheus web界面。 接着下载并安装node_exporter收集器来收集Linux Server性能数据：1234$ wget https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;node_exporter&#x2F;releases&#x2F;download&#x2F;v0.18.1&#x2F;node_exporter-0.18.1.linux-amd64.tar.gz$ tar zxvf node_exporter-0.18.1.linux-amd64.tar.gz$ cd node_exporter-0.18.1.linux-amd64$ .&#x2F;node_exporter 最后下载并安装mysqld_exporter收集器来收集Mysql性能数据，在安装收集器之前需要先在数据库新建用户以提供mysqld_exporter收集器所需的Mysql权限：12GRANT REPLICATION CLIENT,PROCESS ON *.* TO &#39;mysql_monitor&#39;@&#39;localhost&#39; identified by &#39;mysql_monitor&#39;;GRANT SELECT ON *.* TO &#39;mysql_monitor&#39;@&#39;localhost&#39;; 安装mysqld_exporter收集器，并在mysqld_exporter-0.12.1.linux-amd64解压缩后的目录下新增一个名为.my.cnf文件：12345678910$ wget https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;mysqld_exporter&#x2F;releases&#x2F;download&#x2F;v0.12.1&#x2F;mysqld_exporter-0.12.1.linux-amd64.tar.gz$ tar zxvf mysqld_exporter-0.12.1.linux-amd64.tar.gz$ cd mysqld_exporter-0.12.1.linux-amd64$ cat &#x2F;usr&#x2F;local&#x2F;mysqld_exporter-0.10.0.linux-amd64&#x2F;.my.cnf[client]user&#x3D;mysql_monitorpassword&#x3D;mysql_monitor.&#x2F;mysqld_exporter --config.my-cnf&#x3D;&quot;.my.cnf&quot; 最后为了能在Prometheus Server服务器端能够接收node_exporter和mysqld_exporter采集到的数据，需要在prometheus.yml中新增如下配置：1234567891011121314151617scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'server' static_configs: - targets: ['localhost:9100'] - job_name: 'mysql' static_configs: - targets: ['localhost:9104'] 重启Prometheus Server应用后，在Prometheus Web界面中的Status-&gt;Targets页面，可以看到Mysql和L两个Target的状态已经变成UP了： 更多参考：Prometheus中文文档 3. Grafana安装配置Grafana是一款开源的应用分析和监控解决方案。由于Prometheus Web的界面过于简单，为了能够更绚丽的展示Prometheus监控的应用数据，这里选用Grafana实现监控数据的动态可视化组合Dashboard。 首先先下载并运行Grafana：1234$ wget https:&#x2F;&#x2F;dl.grafana.com&#x2F;oss&#x2F;release&#x2F;grafana-6.5.2.linux-amd64.tar.gz $ tar -zxvf grafana-6.5.2.linux-amd64.tar.gz$ cd grafana-6.5.2.linux-amd64$ .&#x2F;bin&#x2F;grafana-server web Grafana启动后，我们可通过http://monitor_host:3000 访问Grafana网页界面（默认登陆帐号/密码为admin/admin）： 接着，新建一个Data Source以从Prometheus接收数据： 然后新建两个Dashboard面板来动态展示Linux和Mysql的监控情况，这里我们可以从Grafana Lab上下载相关应用的Dashboard面板现成样式。Server Dashboard模板： Mysql Dashboard模板： 这里我们只要复制对应的Dashboard的编号，比如上图中Server Dashboard编号是11074和Mysql Dashboard编号是6239。 接下来，在Grafana的Import界面粘贴对应的编号，然后生成Unique identifier (uid)和配置Prometheus Data Source： 最后，生成的Dashboard界面如下：Server Dashboard界面： Mysql Dashboard界面： 4. 结语至此，整个Prometheus+Grafana监控平台搭建与配置完成。如有疑问欢迎留言！ References Prometheus DOCS; prometheus+grafana监控设置; Prometheus+Grafana监控系统搭建.","categories":[{"name":"其他","slug":"Other","permalink":"https://3pigsgu.github.io/info-tech-tutorial/categories/Other/"}],"tags":[{"name":"其他","slug":"Other","permalink":"https://3pigsgu.github.io/info-tech-tutorial/tags/Other/"}]},{"title":"Apache Flink三类API接口","slug":"flink-tutorial-02","date":"2019-12-30T07:46:29.000Z","updated":"2020-01-04T17:19:41.566Z","comments":true,"path":"2019/12/30/flink-tutorial-02/","link":"","permalink":"https://3pigsgu.github.io/info-tech-tutorial/2019/12/30/flink-tutorial-02/","excerpt":"","text":"本篇主要从Flink的三类API接口（DataStream API, DataSet API和Table API）去演示Word Count应用。希望能通过这三个示例代码帮助大家更好的理解这三类API接口。 1. DataStream API接口DataStream API接口即是Flink流式处理接口，对无限制数据流进行过滤或聚合等转换。 1.1. 主要依赖对于DataStream API接口主要依赖flink-streaming-java_2.11这个jar包。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.9.1&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; 1.2. 代码实现首先，我们要创建一个StreamExecutionEnvironment env流式运行环境；然后，获取要输入的数据流的源数据DataStreamSource source，这里模拟输入一段文字；接着，对流式数据进行实时数据处理，通过表达式”\\W+”拆解单个单词并构造成2-tuples（二元组元）集合；再者通过keyBy( 0 )和sum( 1 )对二元组按照下标0的元素分组和下标1的元素求和；最后，通过print()打印出最终流式数据处理结果。这里最重要的是所有上面构建的过程都需要通过env.execute()来开启任务的执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.typeutils.TupleTypeInfo;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;public class WordCountStreaming &#123; public static void main( String[] args ) throws Exception&#123; // set up the execution environment StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data DataStreamSource&lt;String&gt; source = env.fromElements( \"To be, or not to be,--that is the question:--\", \"Whether 'tis nobler in the mind to suffer\", \"The slings and arrows of outrageous fortune\", \"Or to take arms against a sea of troubles\" ); source // split up the lines in pairs (2-tuples) containing: (word,1) .flatMap( ( String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out ) -&gt; &#123; // emit the pairs for( String token : value.toLowerCase().split( \"\\\\W+\" ) )&#123; if( token.length() &gt; 0 )&#123; out.collect( new Tuple2&lt;&gt;( token, 1 ) ); &#125; &#125; &#125; ) // due to type erasure, we need to specify the return type .returns( TupleTypeInfo.getBasicTupleTypeInfo( String.class, Integer.class ) ) // group by the tuple field \"0\" .keyBy( 0 ) // sum up tuple on field \"1\" .sum( 1 ) // print the result .print(); // start the job env.execute(); &#125;&#125; 2. DataSet API接口DataSet API接口即是Flink批量处理接口，可批量对数据集进行转换。 2.1. 主要依赖对于DataSet API接口主要依赖flink-java和flink-clients_2.11这个两个jar包。 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt; &lt;version&gt;1.9.1&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.9.1&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; 2.2. 代码实现整个代码构造过程与DataStream流式处理的过程一致，区别在于这里构造的执行环境是ExecutionEnvironment env，输入的是数据集DataSet text。 123456789101112131415161718192021222324252627282930313233import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.aggregation.Aggregations;import org.apache.flink.api.java.tuple.Tuple2;public class WordCount &#123; public static void main( String[] args ) throws Exception&#123; // set up the execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // input data // you can also use env.readTextFile(...) to get words DataSet&lt;String&gt; text = env.fromElements( \"To be, or not to be,--that is the question:--\", \"Whether 'tis nobler in the mind to suffer\", \"The slings and arrows of outrageous fortune\", \"Or to take arms against a sea of troubles,\" ); DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts = // split up the lines in pairs (2-tuples) containing: (word,1) text.flatMap( new LineSplitter() ) // group by the tuple field \"0\" and sum up tuple field \"1\" .groupBy( 0 ) .aggregate( Aggregations.SUM, 1 ); // emit result counts.print(); &#125;&#125; 数据处理中构造了一个LineSplitter类去实现与Flink流式处理接口相同的工作。 123456789101112131415161718import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.util.Collector;public class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap( String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out )&#123; // normalize and split the line into words String[] tokens = value.toLowerCase().split( \"\\\\W+\" ); // emit the pairs for( String token : tokens )&#123; if( token.length() &gt; 0 )&#123; out.collect( new Tuple2&lt;String, Integer&gt;( token, 1 ) ); &#125; &#125; &#125;&#125; 3. Table API接口Table API接口即是Flink类似于SQL的表达语言的数据处理接口，可以嵌入批处理和流应用程序中一起使用。 3.1. 主要依赖对于Table API接口主要依赖下列这些jar包。 1234567891011121314151617181920212223242526272829&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-api-java&lt;&#x2F;artifactId&gt; &lt;version&gt;1.9.1&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-common&lt;&#x2F;artifactId&gt; &lt;version&gt;1.9.1&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;&#x2F;artifactId&gt; &lt;version&gt;1.9.1&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-planner_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.9.1&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;1.9.1&lt;&#x2F;version&gt; &lt;scope&gt;provided&lt;&#x2F;scope&gt;&lt;&#x2F;dependency&gt; 3.2. 代码实现整个代码构造过程与前面两种处理方式的过程其实是一致的，不同点在于这中间需要实现BatchTableEnvironment表操作环境，而处理完的数据（即拆分后单词）会存储在创建的”words”表中，其字段名称叫做”word”。最后通过SQL语句的方式从”word”表中查询汇总单词数量。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.operators.DataSource;import org.apache.flink.api.java.operators.FlatMapOperator;import org.apache.flink.table.api.Table;import org.apache.flink.table.api.TableConfig;import org.apache.flink.table.api.java.BatchTableEnvironment;import org.apache.flink.types.Row;import org.apache.flink.util.Collector;public class WordCountTable &#123; public static void main( String[] args ) throws Exception&#123; // set up the execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); //final BatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env); final BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env, TableConfig.getDefault()); // get input data DataSource&lt;String&gt; source = env.fromElements( \"To be, or not to be,--that is the question:--\", \"Whether 'tis nobler in the mind to suffer\", \"The slings and arrows of outrageous fortune\", \"Or to take arms against a sea of troubles\" ); // split the sentences into words FlatMapOperator&lt;String, String&gt; dataset = source .flatMap( ( String value, Collector&lt;String&gt; out ) -&gt; &#123; for( String token : value.toLowerCase().split( \"\\\\W+\" ) )&#123; if( token.length() &gt; 0 )&#123; out.collect( token ); &#125; &#125; &#125; ) // with lambdas, we need to tell flink what type to expect .returns(String.class); // create a table named \"words\" from the dataset tableEnv.registerDataSet( \"words\", dataset, \"word\" ); // word count using an sql query Table results = tableEnv.sqlQuery( \"select word, count(*) from words group by word\" ); tableEnv.toDataSet( results, Row.class ).print(); &#125;&#125; 4. 代码执行使用flink命令行工具（在flink安装的bin文件夹中）启动程序： 1flink run -c your.package.WordCount target/your-jar.jar -c选项允许您指定要运行的类。如果jar是可执行的/定义了主类，则没有必要。运行结果如下： 1234567891011121314151617181920212223242526(a,1)(against,1)(and,1)(arms,1)(arrows,1)(be,2)(fortune,1)(in,1)(is,1)(mind,1)(nobler,1)(not,1)(of,2)(or,2)(outrageous,1)(question,1)(sea,1)(slings,1)(suffer,1)(take,1)(that,1)(the,3)(tis,1)(to,4)(troubles,1)(whether,1) References： Getting Started with Apache-Flink.","categories":[{"name":"Flink实战","slug":"Flink-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/categories/Flink-Action/"}],"tags":[{"name":"框架实战","slug":"Architecture-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"}]},{"title":"Apache Flink实战全集","slug":"flink/flink-tutorial-00","date":"2019-12-28T05:21:46.000Z","updated":"2020-01-03T05:29:51.948Z","comments":true,"path":"2019/12/28/flink/flink-tutorial-00/","link":"","permalink":"https://3pigsgu.github.io/info-tech-tutorial/2019/12/28/flink/flink-tutorial-00/","excerpt":"","text":"1. Apache Flink简介1.1. Flink的用途像Apache Hadoop和Apache Spark一样，Apache Flink是一个社区驱动的开源框架，用于分布式大数据分析。 Flink用Java编写，具有适用于Scala，Java和Python的API，可进行批处理和实时流分析。 1.2. 必备环境• 类似于UNIX的环境，例如Linux，Mac OS X或Cygwin(实际上也支持Windows环境)；• Java 6.X或更高版本；• [可选] Maven 3.0.4或更高版本。 1.3. 体系结构和技术栈 1.3.1. Flink应用执行环境Apache Flink是一个数据处理系统，是Hadoop MapReduce组件的替代产品。它带有自己的运行时，而不是在MapReduce之上构建。因此，它可以完全独立于Hadoop生态系统工作。ExecutionEnvironment是执行程序的上下文。您可以根据需要使用不同的环境。 JVM环境：Flink可以在单个Java虚拟机上运行，​​从而允许用户直接从其IDE测试和调试Flink程序。使用此环境时，您需要的只是正确的Maven依赖项。 本地环境：为了能够在正在运行的Flink实例上运行程序（而不是从IDE内部），您需要在计算机上安装Flink。 集群环境：以standalone集群或yarn集群的完全分布式方式运行Flink。 1.3.2. Flink API接口Flink可用于流或批处理。它们提供了三个API： DataStream API：流处理，即对无限制数据流的转换（过滤器，时间窗口，聚合）。 DataSet API：批处理，即数据集的转换。 Table API：类似于SQL的表达语言（例如Spark中的数据框），可以嵌入批处理和流应用程序中。 1.3.3. 构建层次在最基础的层级上，Flink由Sources，Transformations和Sinks组成。 Data Source：Flink处理的传入数据； Transformations：Flink修改传入数据时的处理步骤； Data Sink：Flink处理后在哪里发送数据。 Sources和Sinks可以是本地/HDFS文件，数据库，消息队列等。已经有许多第三方连接器可用，或者您可以轻松创建自己的连接器。 2. Flink实战章节第一篇： Apache Flink快速入门第二篇： Apache Flink三类API接口 Reference List Apache Flink; Apache Flink Documentation; Getting Started with Apache-Flink.","categories":[{"name":"Flink实战","slug":"Flink-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/categories/Flink-Action/"}],"tags":[{"name":"框架实战","slug":"Architecture-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"}]},{"title":"Apache Flink快速入门","slug":"flink-tutorial-01","date":"2019-12-27T18:09:52.000Z","updated":"2020-01-02T15:11:19.262Z","comments":true,"path":"2019/12/28/flink-tutorial-01/","link":"","permalink":"https://3pigsgu.github.io/info-tech-tutorial/2019/12/28/flink-tutorial-01/","excerpt":"","text":"本篇的目的是带Apache Flink初学者引入Flink的大门，并提供一个Word Count的示例来了解如何使用Flink框架，希望能给各位开发爱好者提供浅显易懂的理解。 1. Java运行环境Apache Flink是基于Java语言开发的，并且能运行在Windows, Linux和Mac OS操作系统上。为了能正常运行Flink, 唯一的前提条件是确保安装了Java 6或更高版本的版本，并且已设置JAVA_HOME环境变量。 2. Flink在Windows环境下运行如果要在Windows计算机上本地运行Flink，则需要下载Flink二进制发行版并解压缩。之后，您可以使用Windows批处理文件（.bat），也可以使用Cygwin运行Flink Jobmanager。 (Flink下载地址) 2.1. 以.bat文件启动要从Windows命令行启动Flink，请打开命令窗口，导航到Flink的bin/目录，然后运行start-cluster.bat。注意：Java运行时环境的bin文件夹必须包含在Window的％PATH％变量中。 123456$ cd flink$ cd bin$ start-cluster.batStarting a local cluster with one JobManager process and one TaskManager process.You can terminate the processes via CTRL-C in the spawned shell windows.Web interface by default on http://localhost:8081/. 然后，您需要打开另一个终端使用flink.bat运行作业。 2.2. 安装Cygwin以Unix脚本启动使用Cygwin，您需要启动Cygwin终端，导航到Flink目录并运行start-cluster.sh脚本： 123$ cd flink$ bin/start-cluster.shStarting cluster. 3. Flink在非Windows环境下运行 在此处下载最新的flink二进制文件, 可以选择任意的scala版本：例如Apache Flink 1.9.1 for Scala 2.12。如果您打算使用Hadoop，请选择hadoop相关构件版本。 解压文件压缩包并启动Flink：12tar xzvf flink-1.9.1-bin-scala_2.12.tar.gz #Unpack the downloaded archive./flink/bin/start-cluster.sh #Start Flink Flink已配置为在本地运行。要确保flink正在运行，您可以检查flink/log/中的日志，或打开在http：//localhost：8081上运行的flink jobManager的界面。 停止Flink：1./flink/bin/stop-cluster.sh 4. Flink开发环境要从您的IDE运行flink程序（我们可以使用Eclipse或Intellij IDEA（推荐）），您需要两个依赖项：flink-java/flink-scala和flink-clients（自2016年2月起）。可以使用Maven和SBT添加这些JARS（如果使用的是Scala）。这里我们只介绍如何使用Java进行获取依赖项，而且后面的代码也同样基于Java程序作为示例。核心的Maven依赖包括如下： 12345678910111213141516171819202122232425262728&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;&#x2F;project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;&#x2F;java.version&gt; &lt;flink.version&gt;1.9.1&lt;&#x2F;flink.version&gt; ...&lt;&#x2F;properties&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; &lt;artifactId&gt;flink-connector-wikiedits_2.11&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; 5. Socket Window Word Count示例代码您可以在GitHub上找到此SocketWindowWordCount示例的完整Java版源代码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.common.functions.ReduceFunction;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the port to connect to final int port; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); port = params.getInt(\"port\"); &#125; catch (Exception e) &#123; System.err.println(\"No port specified. Please run 'SocketWindowWordCount --port &lt;port&gt;'\"); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(\"localhost\", port, \"\\n\"); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; @Override public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split(\"\\\\s\")) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy(\"word\") .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; @Override public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute(\"Socket Window WordCount\"); &#125; // Data type for words with count public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + \" : \" + count; &#125; &#125;&#125; 更多参考：Flink官方示例代码 5. 运行Socket Window Word Count示例接下来，我们来运行演示名为SocketWindowWordCount的Flink应用示例。 首先，我们使用netcat在CMD命令窗口中通过以下方式启动本地服务：1$ nc -l -p 9000 然后，通过flink run命令提交Flink程序：12$ ./bin/flink run SocketWindowWordCount.jar --port 9000Starting execution of program 程序连接到socket套接字并等待输入。您可以检查Web界面以验证作业是否按预期运行： 接着我在CMD命令窗口输入下面数个单词：1234$ nc -l -p 9000lorem ipsumipsum ipsum ipsumbye 我们会在启动的Flink集群的任一个窗口看到如下输出：123lorem : 1bye : 1ipsum : 4 这里如果遇到应用程序报Flink Connection refused错误，请参考如下链接： connection failed when running flink in a cluster Unable to run flink example program,Connection refused Flink Connection refused: localhost/127.0.0.1:8081 References： Apache Flink Documentation; Getting Started with Apache-Flink.","categories":[{"name":"Flink实战","slug":"Flink-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/categories/Flink-Action/"}],"tags":[{"name":"框架实战","slug":"Architecture-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"}]},{"title":"Tensorflow安装详解与样例Demo演示","slug":"tensorflow/tensorflow-tutorial-01","date":"2019-12-26T03:40:37.000Z","updated":"2020-01-02T15:11:39.216Z","comments":true,"path":"2019/12/26/tensorflow/tensorflow-tutorial-01/","link":"","permalink":"https://3pigsgu.github.io/info-tech-tutorial/2019/12/26/tensorflow/tensorflow-tutorial-01/","excerpt":"","text":"在使用Tensorflow进行机器学习之前， 我们需要在自己的机器上先安装相关的软件。我们可以安照Tensorflow官方网站上提供的步骤，一步一步的进行安装。官网上会提供多种环境下的部署方法，包括：Linux， Windows，MacOS和Docker容器等等。一般建议在容器或者Virtualenv这样的虚拟环境下进行Tensorflow的安装以及与此配套的一些第三方软件的集成。比较好的一种方式是对于生产环境可以采用Docker这类的容器进行Tensorflow的部署，而对于开发环境可以采用Conda的依赖环境进行安装。 这样可以保证不同版本之间的强依赖关系，避免依赖导致的不必要的冲突发生。 这篇文章将一步一步的带大家在Conda的依赖环境下安装Tensorflow和其周边的第三方插件， 并且演示一个使用Tensorflow的小示例。接下来，我在Windows设备上操作整个过程。 1. Minconda安装Minconda和Conda的区别在于Minconda其实就是Conda的一个子集，是Conda的一个简化安装版： 如果您选择Anaconda，可能：是conda或Python的新手，喜欢一次性安装Python和150多个科学软件包。有时间和磁盘空间-可能需要几分钟和300MB的磁盘空间。不想单独安装要使用的每个软件包。 如果您选择Miniconda，可能：不介意安装要单独使用的每个软件包。没有时间或磁盘空间来一次安装150个以上的软件包。希望快速访问Python和conda命令，并且希望以后再整理其他程序。 这里提供Miniconda安装软件的下载地址, 请依据自己的喜好和机器环境酌情选择。如果你下直接安装完整的Conda软件，也可以在这个链接下载。详细的操作文档请参见：Conda文档。 1.1. 管理Conda 通查看Conda的版本验证Conda是否安装： 1C:\\Users\\44355&gt; conda --version 或者 1C:\\Users\\44355&gt; conda -V Conda会返回版本号，例如：conda 4.8.0。 查看更详细的Conda信息： 1C:\\Users\\44355&gt; conda info Conda返回更多的安装信息，包括安装路径、Conda版本、Python版本、基础环境路径、包缓存路径等。 123456789101112131415161718192021222324252627 active environment : base active env location : D:\\software\\Miniconda3 shell level : 1 user config file : C:\\Users\\44355\\.condarcpopulated config files : C:\\Users\\44355\\.condarc conda version : 4.8.0 conda-build version : not installed python version : 3.7.3.final.0 virtual packages : base environment : D:\\software\\Miniconda3 (writable) channel URLs : https://repo.anaconda.com/pkgs/main/win-64 https://repo.anaconda.com/pkgs/main/noarch https://repo.anaconda.com/pkgs/r/win-64 https://repo.anaconda.com/pkgs/r/noarch https://repo.anaconda.com/pkgs/msys2/win-64 https://repo.anaconda.com/pkgs/msys2/noarch package cache : D:\\software\\Miniconda3\\pkgs C:\\Users\\44355\\.conda\\pkgs C:\\Users\\44355\\AppData\\Local\\conda\\conda\\pkgs envs directories : D:\\software\\Miniconda3\\envs C:\\Users\\44355\\.conda\\envs C:\\Users\\44355\\AppData\\Local\\conda\\conda\\envs platform : win-64 user-agent : conda/4.8.0 requests/2.22.0 CPython/3.7.3 Windows/10 Windows/10.0.17763 administrator : False netrc file : None offline mode : False 更新Conda到最新版本： 1C:\\Users\\44355&gt; conda update conda 更多参考：Conda Management 1.2. Conda的环境管理 命令行创建一个新环境 1C:\\Users\\44355&gt; conda create --name tensorflow-env 命令行激活指定环境 1C:\\Users\\44355&gt; conda activate tensorflow-env 激活环境后所有的操作会在当前激活的环境下进行。 命令行停用当前环境 1(tensorflow-env) C:\\Users\\44355&gt; conda deactivate 更多参考：Conda Environment Management 2. 在tensorflow-env环境下进行Python安装 当我们在Conda中激活tensorflow-env环境，则出现如下提示：1(tensorflow-env) C:\\Users\\44355&gt; 此时表明当前已经进入tensorflow-env环境。 由于在安装Conda的时候其自身内嵌安装了相应的Python版本，因此我们无需再额外安装Python,就可以直接在tensorflow-env环境中安装Tensorflow。首先查看Tensorflow各个版本：1(tensorflow-env) C:\\Users\\44355&gt; conda search tensorflow 或者1(tensorflow-env) C:\\Users\\44355&gt; conda search tensorflow-gpu 选择一个安装版本进行安装：1(tensorflow-env) C:\\Users\\44355&gt; conda install tensorflow=1.8.0 或者直接安装最新版本的Tensorflow：1(tensorflow-env) C:\\Users\\44355&gt; conda install tensorflow 安装完成后验证Tensorflow是否安装成功：1234(tensorflow-env) C:\\Users\\44355&gt; python&gt;&gt;&gt; import tensorflow as tf #不报错即安装成功&gt;&gt;&gt; exit()(tensorflow-env) C:\\Users\\44355&gt; conda deactivate 更多参考：Tensorflow官方安装 3. 在tensorflow-env环境下安装常用的第三方软件3.1. 安装Jupyter Jupyter是一款为了开发跨多种编程语言的交互式计算的开源软件，它提供开放的标准和服务。JupyterLab是用于Jupyter笔记本，代码和数据的基于Web的交互式开发环境。 JupyterLab非常灵活：配置和安排用户界面以支持数据科学，科学计算和机器学习中的各种工作流程。 JupyterLab是可扩展的和模块化的：编写可添加新组件并与现有组件集成的插件。接下来我们通过pip命令来安装JupyterLab。 1(tensorflow-env) C:\\Users\\44355&gt; pip install jupyterlab 更多参考：Jupyter官方安装 Jupyter Notebook是Jupyter中的一个开源Web应用程序，允许您创建和共享包含实时代码，方程式，可视化效果和叙述文本的文档。用途包括：数据清理和转换，数值模拟，统计建模，数据可视化，机器学习等。我们可以通过命令来启动Jupyter Notebook服务。 1(tensorflow-env) C:\\Users\\44355&gt; jupyter notebook 终端上会打印如下启动信息： 123456789101112[I 10:20:26.155 NotebookApp] Serving notebooks from local directory: C:\\Users\\44355[I 10:20:26.160 NotebookApp] The Jupyter Notebook is running at:[I 10:20:26.161 NotebookApp] http://localhost:8888/?token=00f06b6e31b020740277e2d20eef416da634cc55d6f6a2ba[I 10:20:26.162 NotebookApp] or http://127.0.0.1:8888/?token=00f06b6e31b020740277e2d20eef416da634cc55d6f6a2ba[I 10:20:26.162 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[C 10:20:26.406 NotebookApp] To access the notebook, open this file in a browser: file:///C:/Users/44355/AppData/Roaming/jupyter/runtime/nbserver-138920-open.html Or copy and paste one of these URLs: http://localhost:8888/?token=00f06b6e31b020740277e2d20eef416da634cc55d6f6a2ba or http://127.0.0.1:8888/?token=00f06b6e31b020740277e2d20eef416da634cc55d6f6a2ba 接下来会自动打开浏览器进入Jupyter Notebook操作界面： 更多参考：Jupyter官方文档 3.2. 安装MatplotlibMatplotlib是一个Python 2D绘图库，它以多种硬拷贝格式和跨平台的交互式环境生成出版物质量的图形。 Matplotlib可用于Python脚本，Python和IPython Shell，Jupyter Notebook，Web应用程序服务器和四个图形用户界面工具包。接下来我们通过Python来安装Matplotlib插件： 12(tensorflow-env) C:\\Users\\44355&gt; python -m pip install -U pip(tensorflow-env) C:\\Users\\44355&gt; python -m pip install -U matplotlib 更多参考：Matplotlib官方安装现在我们只需几行代码就可以生成图表，直方图，功率谱，条形图，误差图，散点图等。 4. 示例代码演示TensorFlow, Jupyter Notebook, and Matplotlib的使用接下来，我们在Jupyter Notebook界面中写一段很简单的演示代码。首先我们在Jupyter Notebook操作界面的右上角单击 new 按钮，然后选择Python版本，这边选择的是安装Conda时自带的Python3. 进入代码操作界面后，可以把默认代码文件名Untitled修改成自己定义的名字，这里修改成“My First Notebook”。 然后我们在代码块区域输入如下示例代码： 1234567891011import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinea = tf.random_normal([2,20])sess = tf.Session()out = sess.run(a)x, y = outplt.scatter(x, y)plt.show() 代码中tf.random_normal()函数用于从服从指定正太分布的数值中取出指定个数的值。 shape=[2,20]是输出张量的形状，一个2行20列的矩阵。然后tensorflow运行计算后的输出按行分别传给x和y两个变量。最后以Matplotlib的scatter散点图的方式图形化展示，其中x代表横轴值，y代表纵轴值。代码可以分成多个代码块，代码块可以通过左上角 + 号增加，最终完成界面如下： 完成Python示例代码后，可以点击 Run 按钮逐步执行代码块，这里代码调用tensorflow正太分布算法，最终结果会使用Matplotlib去直观的展示20个点的随机正太分布情况。 4. 结语至此整个Tensorflow的安装详解和示例代码演示到此结束，如果需要了解更多的相关细节，请阅读本文中提供的更多的官方链接。如有疑问欢迎大家留言！欢迎来踩留印！","categories":[{"name":"Tensorflow实战","slug":"Tensorflow-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/categories/Tensorflow-Action/"}],"tags":[{"name":"框架实战","slug":"Architecture-Action","permalink":"https://3pigsgu.github.io/info-tech-tutorial/tags/Architecture-Action/"}]}]}